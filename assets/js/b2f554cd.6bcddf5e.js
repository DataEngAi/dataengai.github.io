"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"temporal-join","metadata":{"permalink":"/blog/temporal-join","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-07-10-temporal-join.md","source":"@site/blog/2023-07-10-temporal-join.md","title":"Why Temporal Join is Stream Processing\u2019s Superpower","description":"Stream processing technologies like Apache Flink introduce a new type of data transformation that\u2019s very powerful: the temporal join. Temporal joins add context to data streams while being efficient and fast to execute.","date":"2023-07-10T00:00:00.000Z","formattedDate":"July 10, 2023","tags":[{"label":"Join","permalink":"/blog/tags/join"},{"label":"Flink","permalink":"/blog/tags/flink"},{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":9.575,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"Founder of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"temporal-join","title":"Why Temporal Join is Stream Processing\u2019s Superpower","authors":["matthias"],"tags":["Join","Flink","DataSQRL"]},"nextItem":{"title":"Why Apache Flink and GraphQL Are like Peanut Butter and Jelly","permalink":"/blog/flink-graphql-peanut-butter-jelly"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/dev/temporal_join.png\\" />\\n</head>\\n\\nStream processing technologies like Apache Flink introduce a new type of data transformation that\u2019s very powerful: the temporal join. Temporal joins add context to data streams while being efficient and fast to execute.\\n\\n<img src=\\"/img/dev/temporal_join.svg\\" alt=\\"Temporal Join >\\" width=\\"30%\\"/>\\n\\nThis article introduces the temporal join, compares it to the traditional inner join, explains when to use it, and why it is a secret superpower.\\n\\nTable of Contents:\\n* [The Join: A Quick Review](#review)\\n* [The Temporal Join: Linking Stream and State](#tempjoin)\\n* [Temporal Join vs Inner Join](#tempinner)\\n* [Why Temporal Joins are Fast and Efficient](#efficient)\\n* [Temporal Joins Made Easy to Use](#easy)\\n* [Summary](#summary)\\n\\n\x3c!--truncate--\x3e\\n\\n## The Join: A Quick Review {#review}\\n\\nLet\'s take a quick detour down memory lane and revisit the good ol\' join operation. That trusty sidekick in your SQL utility belt helps you link data from two or more tables based on a related column between them.\\n\\nSuppose we are operating a factory with a number of machines that roast and package coffee. We place sensors on each machine to monitor the temperature and detect overheating.\\n\\nWe keep track of the sensors and machines in two database tables.\\n\\nThe `Sensor` table contains the serial number and machine id that the sensor is placed on.\\n\\n| id    | serialNo | machineid |\\n|-------|----------|----------|\\n| 11503 | X57-774  | 203      |\\n| 11523 | X33-453  | 203      |\\n| 11034 | X54-554  | 501      |\\n\\nThe `Machine` table contains the name of each machine.\\n\\n| id    | name           |\\n|-------|----------------|\\n| 203   | Iron Roaster   |\\n| 501   | Gritty Grinder |\\n\\nTo identify all the sensors on the machine \u201cIron Roaster\u201d we use the following SQL query which joins the `Sensor` and `Machine` tables:\\n```sql\\nSELECT s.id, s.serialNo FROM Sensor s \\n    JOIN Machine m ON s.machineid = m.id \\n    WHERE m.name = \u201cIron Roaster\u201d\\n```\\n\\nWhy are joins as important as your morning coffee? Without it, your data tables are like islands, isolated and lonely. Joins bring them together, creating meaningful relationships between data, and enriching data records with context to see the bigger picture.\\n\\nBy default, databases execute joins as **inner** joins which means only matching records are included in the join.\\n\\nSo, now that we\'ve refreshed our memory about the classic join, let\'s dive into the exciting world of temporal joins in stream processing systems like Apache Flink.\\n\\n## The Temporal Join: Linking Stream and State {#tempjoin}\\n\\n<img src=\\"/img/blog/delorean.jpeg\\" alt=\\"Temporal Join DeLorean >\\" width=\\"40%\\"/>\\n\\nPicture this: you\'re a time traveler. You have the power to access any point in time, past or future, at your will. Now, imagine that your data could do the same. Enter the Temporal Join, the DeLorean of data operations, capable of taking your data on a time-traveling adventure.\\n\\nA Temporal Join is like a regular join but with a twist. It allows you to join a stream of data (the time traveler) with a versioned table (the timeline) based on the time attribute of the data stream. This means that for each record in the stream, the join will find the most recent record in the versioned table that is less than or equal to the stream record\'s time.\\n\\nThe versioned table is a normal state table where we keep track of data changes over time. That is, we keep older versions of each record around to allow the stream to match the correct version in time. Like time travel, temporal joins can make your head spin a bit. Let\u2019s look at an example to break it down.\\n\\n## Temporal Join vs Inner Join {#tempinner}\\n\\nBack to our coffee roasting factory, we collect the temperature readings from each sensor in a data stream. \\n\\n| timestamp           | sensorid | temperature |\\n|---------------------|----------|-------------|\\n| 2023-07-10T13:25:15 | 11503    | 78.2        |\\n| 2023-07-10T13:25:15 | 11523    | 83.1        |\\n| 2023-07-10T13:25:15 | 11034    | 101.5       |\\n| 2023-07-10T13:25:16 | 11503    | 77.8        |\\n| 2023-07-10T13:25:16 | 11523    | 83.5        |\\n| 2023-07-10T13:25:16 | 11034    | 105         |\\n\\nAnd we want to know the maximum temperature recorded for each machine.\\n\\nEasy enough, let\u2019s join the temperature data stream with the Sensors table and aggregate by machine id:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r INNER JOIN Sensor s \\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nBut here is a problem: What if we moved a sensor from one machine to another during the day? With an inner join, all of the sensor\u2019s readings would be linked to the machine it was last placed on. So, if sensor 1 records a high temperature of 105 degrees in the morning and we move the sensor to the \u201cIron Roaster\u201d machine in the afternoon, then we might see the 105 degrees falsely show up as the maximum temperature for the Iron Roaster. See how time played a trick on our join?\\n\\nAnd this happens whenever we join a data stream with a state table that changes over time, like our sensors that get moved around the factory. What to do? Let\u2019s call the temporal join to our rescue:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r TEMPORAL JOIN Sensor s \\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nPretty much the same query, just a different join type. As a temporal join, we are joining each sensor reading with the version of the sensor record at the time of the data stream. In other words, the join not only matches the sensor reading with the sensor record based on the id but also based on the timestamp of the reading to ensure it matches the right version of the sensor record. Pretty neat, right?\\n\\nWhenever you join a data stream with a state that changes over time, you want to use the temporal join to make sure your data is lined up correctly in time. Temporal joins are a powerful feature of stream processing engines that would be difficult to implement in a database ([see appendix](#appendix)).\\n\\n\\n## Why Temporal Joins are Fast and Efficient {#efficient}\\n\\n<img src=\\"/img/external/flink_logo.svg\\" alt=\\"Apache Flink >\\" width=\\"30%\\"/>\\n\\nNot only do temporal joins solve the time-alignment problem when joining data streams with changing state, modern stream processors like Apache Flink are also incredibly efficient at executing temporal joins. A powerful feature with great performance? Sounds too good to be true. Let\u2019s peek behind the stream processing curtain to find out why.\\n\\nIn stream processing, joins are maintained as the underlying data changes over time. That requires the stream engine to hold all the data it needs to update join records when either side of the join changes. This makes inner joins pretty expensive on data streams.\\n\\nConsider our max-temperature query with the inner join: When we join a temperature reading with the corresponding sensor record, and that record changes, the engine has to update the result join record. To do so, it has to store all of the sensor readings to determine which join results are affected by a change in a sensor record. This can lead to a lot of updates and hence a lot of downstream computation. It can also cause system failure when there are a lot of temperature readings in our data stream because the stream engine has to store all of them.\\n\\nTemporal joins, on the other hand, can be executed much more efficiently. The stream engine only needs to store the versions of the sensor table that are within the time bounds of the sensor reading data stream. And it only has to briefly store (if at all) the sensor reading records to ensure they are joined with the most up-to-date sensor records. Moreover, temporal joins don\u2019t require sending out a massive amount of updated join records when sensors change placement since the join is fixed in time.\\n\\n## Temporal Joins Made Easy to Use {#easy}\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"30%\\"/>\\n\\nBecause temporal joins are so powerful, we made them easy to use in DataSQRL. DataSQRL is a compiler for Apache Flink that builds integrated microservices for your event-driven or streaming applications. DataSQRL supports the simplified temporal join syntax shown in the queries above. In addition, DataSQRL defaults to a temporal join whenever you join a state and a stream table on the state table\u2019s primary key. In that way, DataSQRL helps you pick the right join for your data and makes it easy for developers new to stream processing.\\n\\n\\nApache Flink also supports temporal joins in Flink SQL using the following syntax:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading AS r  JOIN Sensor \\n    FOR SYSTEM_TIME AS OF SensorReading.timestamp AS s \\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nYou need to be careful that the join column for the state table is the primary key of that table and that you set the timestamp SensorReading table. DataSQRL does that for you automatically.\\n\\n## Time to Wrap Up This Temporal Journey {#summary}\\n\\nWe\'ve reached the end of our time-traveling adventure through the universe of temporal joins. We\'ve seen how they\'re like the DeLorean of data operations, zipping us back and forth through time to make sure our data matches up just right. We\'ve also compared them to the good ol\' inner join.\\n\\nTemporal joins help us avoid the pitfalls of time-alignment problems when joining data streams with changing state. They\'re also super efficient, making them a great choice for high-volume, real-time data processing.\\n\\nAnd that\u2019s why the temporal join is stream processing\'s secret superpower.\\n\\nDataSQRL makes using temporal joins a breeze. With its simplified syntax and smart defaults, it\'s like having a personal tour guide leading you through the sometimes bewildering landscape of stream processing. Take a look at our [IoT tutorial](/docs/getting-started/tutorials/iot/intro/) to see a complete example of temporal joins in action or take a look at our [extended tutorial](/docs/getting-started/intro/overview) for a step-by-step guide to stream processing including temporal joins. And we are [here to help](/community) if you have any questions.\\n\\nHappy data time-traveling, folks!\\n\\n## Appendix: Temporal Joins in Relational Databases {#appendix}\\n\\nIt is possible to implement temporal joins in relational databases but it requires a bit of work.\\n\\nWe have to store the version history of the `Sensor` table so that we can join each sensor reading with the \u201ccorrect\u201d sensor placement at the time of the reading. That means we need to add a `versionTime` column to our `Sensor` table. Then we have to write a nested query to identify the correct version of the `Sensor` table to join with:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r JOIN \\n    (SELECT id, machineid, versionTime as start_time, LEAD(versionTime) \\n        OVER (PARTITION BY id ORDER BY versionTime) as end_time FROM Sensor) AS s \\nON (s.id = r.sensorid AND s.start_time <= r.timestamp AND (s.end_time IS NULL OR s.end_time > r.timestamp)) \\nGROUP BY s.machineid\\n```\\n\\nCompared to stream processors like Apache Flink, the database version of temporal joins is more complex and has performance issues.\\n\\nIt\u2019s more complex because we have to use a nested SELECT query that uses windowing to identify the correct version for each sensor based on the `versionTime` column.\\n\\nIt has performance issues because the nested SELECT query and the join are expensive to execute as the version table grows over time. That means we have to carefully tune the query and make sure we periodically prune the sensor version table to remove outdated versions.\\nApache Flink uses watermarks to automatically drop outdated versions without any extra effort on our part. And a temporal join in Apache Flink is a simple lookup."},{"id":"flink-graphql-peanut-butter-jelly","metadata":{"permalink":"/blog/flink-graphql-peanut-butter-jelly","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-06-27-flink-graphql.md","source":"@site/blog/2023-06-27-flink-graphql.md","title":"Why Apache Flink and GraphQL Are like Peanut Butter and Jelly","description":"In the world of data-driven applications, Apache Flink is a powerful tool that transforms streams of raw data into valuable results. But how do you make these results accessible to users, customers, or consumers of your application? Most often, we found the answer to that question was: GraphQL. GraphQL gives users a flexible way to query for data, makes it easy to ingest events, and supports pushing data updates to the user in real-time.","date":"2023-06-27T00:00:00.000Z","formattedDate":"June 27, 2023","tags":[{"label":"GraphQL","permalink":"/blog/tags/graph-ql"},{"label":"Flink","permalink":"/blog/tags/flink"},{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":8.47,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"Founder of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"flink-graphql-peanut-butter-jelly","title":"Why Apache Flink and GraphQL Are like Peanut Butter and Jelly","authors":["matthias"],"tags":["GraphQL","Flink","DataSQRL"]},"prevItem":{"title":"Why Temporal Join is Stream Processing\u2019s Superpower","permalink":"/blog/temporal-join"},"nextItem":{"title":"Simplifying Apache Flink Application Development with DataSQRL","permalink":"/blog/simplifying-flink-app-development"}},"content":"In the world of data-driven applications, Apache Flink is a powerful tool that transforms streams of raw data into valuable results. But how do you make these results accessible to users, customers, or consumers of your application? Most often, we found the answer to that question was: GraphQL. GraphQL gives users a flexible way to query for data, makes it easy to ingest events, and supports pushing data updates to the user in real-time.\\n\\n<img src=\\"/img/blog/flink_graphql.svg\\" alt=\\"Flink hearts GraphQL >\\" width=\\"40%\\"/>\\n\\nIn this blog post, we\u2019ll discuss what GraphQL is and why it is a good fit for Flink applications. Like peanut butter and jelly, Flink and GraphQL don\u2019t seem related but the combination is surprisingly good.\\n\\nTable of Contents:\\n * [How To Access Flink Results?](#access)\\n * [What is GraphQL?](#graphql)\\n * [Benefit #1: Flexible Access for Data APIs](#benefit1)\\n * [Benefit #2: Realtime Data Updates with GraphQL Subscriptions](#benefit2)\\n * [Benefit #3: Simplify Event Ingestion with GraphQL Mutations](#benefit3)\\n * [Downsides of using GraphQL in Flink Applications](#downsides)\\n * [How to Build GraphQL APIs with Flink](#howto)\\n\\n\\n## How To Access Flink Results? {#access}\\n\\nQuick background before we dive into the details. [Apache Flink](https://flink.apache.org/) is a scalable stream processor that can ingest data from multiple sources, integrate, transform, and analyze the data, and produce results in real time. Apache Flink is the brain of your data processing operations.\\n\\n<img src=\\"/img/external/flink_logo.svg\\" alt=\\"Flink Logo >\\" width=\\"30%\\"/>\\n\\nBut Apache Flink cannot make the processed results accessible to users of your application. Flink has an API, but that API is only for administering and monitoring Flink jobs. It doesn\u2019t give outside users access to the result data. In other words, Flink is a brain without a mouth to communicate results externally.\\n\\nTo make results accessible, you have to write them somewhere and expose them through an interface. But how? We have built a number of Flink applications and in most cases, the answer was: write the results to a database or Kafka and expose them through an API. Over the years, our default choice for the API has become GraphQL. Here\u2019s why.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is GraphQL? {#graphql}\\n\\nFirst, let\'s talk about [GraphQL](https://graphql.org/), the data query and manipulation language that\'s been shaking up the API world.\\n\\n<img src=\\"/img/external/graphql_logo.svg\\" alt=\\"GraphQL Logo >\\" width=\\"30%\\"/>\\n\\nGraphQL is a robust alternative to REST for building APIs. It provides a complete and understandable description of your data in the API and gives clients the power to ask for exactly what they need. This means no more over-fetching of data or making multiple requests, which can be an issue with REST.\\n\\nIn contrast to REST, which uses a separate URL for each resource, GraphQL operates over a single endpoint using HTTP. This simplifies the process of making API calls, as you don\'t have to construct multiple URLs. Furthermore, GraphQL uses a type system to describe data, which can make your API calls more predictable and easier to debug.\\n\\nLike REST, GraphQL is mature and well-established with lots of tooling and client libraries for pretty much all programming languages under the sun.\\n\\nOkay, so GraphQL allows you to build APIs that are easy to use. But what makes it such a good match for Apache Flink applications? GraphQL has three key features that unlock the power of Flink\u2019s data processing: flexible queries, subscriptions, and mutations. Let\u2019s look at those in more detail.\\n\\n## 1. Flexible Access for Data APIs {#benefit1}\\n\\nImagine you\'re at an all-you-can-eat buffet, but instead of food, it\'s data. You can pick and choose exactly what you want and how much of it you need. That\'s what GraphQL does for you when it comes to exposing data APIs. Developers can query the processed streaming data however they need it, without being constrained by predefined endpoints.\\n\\nOther API standards like REST or gRPC are like ordering dishes from a menu. You can only access the endpoints that are defined in the API. That\u2019s fine for many applications, but for data APIs, there are many possible combinations of data records users want to query. So, you either define a lot of endpoints or shift the burden onto the user to stitch their desired result sets together. In the end, it\u2019s more work and a lot of food gets wasted.\\n\\nBut wait, there\'s more! GraphQL not only allows you to select the data you need, but it also simplifies the process of combining data from multiple sources. This means you can easily merge data from various microservices and databases, creating a seamless and unified experience for developers.\\n\\nIn a nutshell, GraphQL empowers developers by providing a flexible, powerful, and efficient way to access the data they need.\\n\\n## 2. Realtime Data Updates with GraphQL Subscriptions {#benefit2}\\n\\nRemember those walkie-talkies you used to play with as a kid? You\'d press the button, and your voice would be magically transmitted to your friend\'s walkie-talkie in real-time. Well, GraphQL subscriptions are kind of like that, but for data.\\n\\nSubscriptions allow Flink developers to push real-time data updates to consumers of the API, ensuring that everyone is always up-to-date with the latest information. It\'s like having a walkie-talkie channel dedicated to data updates, so you never miss a beat.\\n\\nBut that\'s not all! GraphQL subscriptions also enable developers to filter and control the data they receive. This means that each consumer can specify exactly what data they want to be updated on, reducing the amount of unnecessary information being transmitted.\\n\\nIn essence, GraphQL subscriptions provide a powerful and efficient way to keep everyone in the loop with real-time data updates. It\'s like having a personal news ticker, tailored specifically to your needs.\\n\\n## 3. Simplify Event Ingestion with GraphQL Mutations  {#benefit3}\\n\\nMutations make it simple to ingest events into your system, streamlining the process and keeping everything running smoothly. GraphQL mutations support complex event payloads that are defined as input types directly in the API specification, making it easy for users of your API to submit data.\\n\\nGraphQL was developed with mobile applications in mind, where the connection between phone and server can be spotty. GraphQL mutations follow an event-centric model to separate the state of the mobile device from the state on the server and ensure continued operations when the connection between the two gets interrupted. That\u2019s a perfect match for event-driven microservices and streaming applications which follow the same model for decoupling.\\n\\nAnd unlike REST, you don\u2019t have to worry about HTTP methods and state management which add a level of complexity you don\u2019t need for event-driven applications.\\n\\nIn summary, GraphQL mutations structure data ingestion and updates as events - just like Flink.\\n\\n## Downsides of using GraphQL in Flink Applications {#downsides}\\n\\nDespite the many advantages of using GraphQL with Apache Flink, it\'s important to note that there are some potential downsides to this approach. Like any technology, GraphQL is not a silver bullet and there are certain challenges that developers may encounter when implementing it in their Flink applications.\\n\\nFirstly, the very flexibility that makes GraphQL so appealing can also present challenges. The ability to request exactly the data you need means that your backend has to be prepared to handle a potentially wide variety of query structures. This can be difficult to implement, and it requires careful consideration of how your database is structured and indexed. We\u2019ve seen a single missing index bring down an entire application because the database became overloaded. Not a fun day.\\n\\nSecondly, while GraphQL\'s subscription model is a powerful tool for delivering real-time updates, it can also be difficult to implement with low latency. Especially in high-volume data environments like those typically handled by Flink. Ensuring that updates are delivered to clients as quickly as possible, without overwhelming the server or the network, can be a complex task that requires careful planning and optimization.\\n\\nHowever, these challenges are not insurmountable. With the right tooling and a thoughtful approach to implementation, it\'s possible to take full advantage of the power and flexibility that GraphQL offers without optimizing the implementation by hand. DataSQRL is a tool that optimizes your Flink jobs as well as generates the database schema and index structures for your GraphQL API, compiling an entire event-driven microservice and saving you a ton of work. More on DataSQRL below.\\n\\n## How to Build GraphQL APIs with Flink {#howto}\\n\\nTo sum up, the combination of Apache Flink and GraphQL provides a potent solution for data-driven applications. Like peanut butter and jelly, these two distinct technologies complement each other and create a powerful synergy. Flink processes the data, and GraphQL exposes it in a flexible, efficient, and real-time manner. GraphQL\u2019s ability to provide flexible access to data APIs, push real-time data updates through subscriptions, and simplify event ingestion with mutations, makes it an ideal interface for Flink\'s data processing capabilities. Together, they enable developers to build robust, efficient, and user-friendly data applications. So, whether you\'re building a real-time analytics platform, a complex event-driven application, or a data-intensive microservice, consider the Flink-GraphQL combo.\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"30%\\"/>\\n\\nBy now you are probably hungry to learn more. It may just be all the food analogies, but if you want to learn how to build GraphQL APIs with Flink then read on.\\n\\nYou can build Flink+GraphQL microservices by hand, but like many things in software, it gets a lot easier with the appropriate tooling. We recommend using [DataSQRL](/) for building GraphQL APIs on top of Flink. DataSQRL compiles the SQL that defines your data processing in Flink and your GraphQL API into a complete microservice that integrates these components efficiently.\\n\\nOf course, we are biased.  But, who knows, DataSQRL might save you a ton of work, time, and frustration.\\n\\nCheck out the [Quickstart tutorial](/docs/getting-started/quickstart) for a quick overview of how to consume and ingest data, process it with Flink, and expose the results through a GraphQL API.\\nLooking for a step-by-step guide to building your own GraphQL APIs with Flink? Check out the [introductory tutorial](/docs/getting-started/intro/overview) for a detailed explanation of all the steps.\\n\\nAnd if you run into any issues or have questions, don\u2019t hesitate to [reach out](/community).\\n\\nHappy coding, and may the match of GraphQL and Apache Flink turn your application into a delicious experience for your users!"},{"id":"simplifying-flink-app-development","metadata":{"permalink":"/blog/simplifying-flink-app-development","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-06-21-simplifying-flink-app-development.md","source":"@site/blog/2023-06-21-simplifying-flink-app-development.md","title":"Simplifying Apache Flink Application Development with DataSQRL","description":"Apache Flink is an incredibly powerful stream processor. But to build a complete application with Flink you need to integrate multiple complex technologies which requires a significant amount of custom code.","date":"2023-06-21T00:00:00.000Z","formattedDate":"June 21, 2023","tags":[{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"},{"label":"Flink","permalink":"/blog/tags/flink"}],"readingTime":4.445,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"Founder of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"simplifying-flink-app-development","title":"Simplifying Apache Flink Application Development with DataSQRL","authors":["matthias"],"tags":["DataSQRL","Flink"]},"prevItem":{"title":"Why Apache Flink and GraphQL Are like Peanut Butter and Jelly","permalink":"/blog/flink-graphql-peanut-butter-jelly"},"nextItem":{"title":"SQRL: Enhancing SQL to a High-Level Data Language","permalink":"/blog/sqrl-high-level-data-language-sql"}},"content":"Apache Flink is an incredibly powerful stream processor. But to build a complete application with Flink you need to integrate multiple complex technologies which requires a significant amount of custom code.\\nDataSQRL is an open-source tool that simplifies this process by compiling SQL into a microservice that integrates Flink, Kafka, Postgres, and API layer. \\n\\n<div style={{float: \'right\', width: \'40%\'}}>\\n<iframe width=\\"100%\\" height=\\"100%\\" src=\\"https://www.youtube.com/embed/mf5q-IdbVQY\\" title=\\"DataSQRL Introduction\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen></iframe>\\n</div>\\n\\nDataSQRL allows you to focus on your application logic without getting bogged down in the details of how to execute your data transformations efficiently across multiple technologies.\\n\\n## The Challenge of Building Applications with Flink\\n\\nWe have built several applications in Flink: recommendation engines, data mesh endpoints, monitoring dashboards, Customer 360 APIs, smart IoT apps, and more. Across those use cases, Flink proved to be versatile and powerful in its ability to instantly analyze and aggregate data from multiple sources. But we found it quite difficult and time-consuming to build applications with Flink.\\n\\n<img src=\\"/img/reference/compiledMicroservice.svg\\" alt=\\"DataSQRL compiled microservice >\\" width=\\"50%\\"/>\\n\\nTo start, you need to learn Flink: the table and datastream API, watermarking, windowing, and all the other stream processing concepts. Flink alone gets our heads spinning. And Flink is just one component of the application.\\n\\nTo build a complete application or microservice, you need Kafka to hold your streaming data and a database like Postgres to query the processed data. On top, you need an API layer that captures input data and provides access to the processed data. Your team must learn, implement, and integrate multiple complex technologies. It takes a village to build a Flink app.\\n\\n## Introducing DataSQRL: A Solution for Streamlined Flink Development\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"20%\\"/>\\n\\nThat\u2019s why we built [DataSQRL](/). DataSQRL compiles the SQL that defines your data processing into an integrated microservice that orchestrates Flink, Kafka, Postgres, and API - saving us a ton of time and headache in the process. Why not let the computer do all the hard work?\\n\\nLet me show you how DataSQRL works by building an IoT monitoring service.\\n\\n\x3c!--truncate--\x3e\\n\\nFirst, we implement the data processing for our monitoring service in SQL.\\n\\n```sql title=metrics.sqrl\\nIMPORT datasqrl.example.sensors.SensorReading; -- Import sensor data\\nIMPORT time.endOfSecond;  -- Import time function\\n/* Aggregate sensor readings to second */\\nSecReading := SELECT sensorid, endOfSecond(time) as timeSec,\\n                     avg(temperature) as temp FROM SensorReading\\n              GROUP BY sensorid, timeSec;\\n/* Get max temperature in last minute */\\nSensorMaxTemp := SELECT sensorid, max(temp) as maxTemp\\n                 FROM SecReading\\n                 WHERE timeSec >= now() - INTERVAL 1 MINUTE\\n                 GROUP BY sensorid;\\n```\\n\\nThis script imports the metrics stream which are temperature readings collected by sensors. DataSQRL treats external data sources like software dependencies that you import, allowing the compiler to handle configuration management, data mapping, and schema evolution for us. We also import a time function we\u2019ll use for aggregation.\\n\\nThen we define two tables: The first table smoothes out the readings by taking the average temperature each second. The SensorMaxTemp table computes the maximum temperature for each sensor over the last minute.\\n\\nNext, we are going to define the API for our monitoring service.  That\u2019s how users of our service query the data.\\nThe API is defined by a GraphQL schema. It specifies the query endpoints and result types.\\n\\n```graphql title=metricsapi.graphqls\\ntype Query {\\n    SecReading(sensorid: Int!): [SecReading!]\\n    SensorMaxTemp(sensorid: Int): [SensorMaxTemp!]\\n}\\n\\n\\ntype SecReading {\\n    sensorid: Int!\\n    timeSec: String!\\n    temp: Float!\\n}\\n\\n\\ntype SensorMaxTemp {\\n    sensorid: Int!\\n    maxTemp: Float!\\n}\\n```\\n\\nNote, how the tables map to query endpoints and types. That\u2019s how it all fits together.\\n\\nAnd don\u2019t worry, you don\u2019t have to write this schema - DataSQRL can generate it for you from the SQL script.\\n\\nDataSQRL compiles the script and GraphQL schema into an integrated microservice with the following command:\\n\\n```bash\\ndocker run --rm -v $PWD:/build datasqrl/cmd compile metrics.sqrl metricsapi.graphqls\\n```\\n\\nIt also generates a docker-compose template to stand up the entire service.\\n\\n```bash\\n(cd build/deploy; docker compose up)\\n```\\n\\nWe can now interact with the API and try it out by opening [http://localhost:8888/graphiql/](http://localhost:8888/graphiql/?query=query%20MaxTemp%20%7B%0A%20%20SensorMaxTemp%20%7B%0A%20%20%20%20sensorid%0A%20%20%20%20maxTemp%0A%20%20%7D%0A%7D%0A&operationName=MaxTemp).\\n\\n## DataSQRL Does the Work for You\\n\\n\\nPretty simple, right? And the best part is that DataSQRL compiles deployment artifacts for each component that you can inspect and deploy anywhere. There is no magic or black box.\\nFor example, DataSQRL compiles a Flink jar you can execute on an existing Flink cluster or Flink managed service.\\n\\n<img src=\\"/img/generic/undraw_launch.svg\\" alt=\\"DataSQRL Does the Work >\\" width=\\"30%\\"/>\\n\\nMost importantly, consider all the work we didn\u2019t have to do. No data source configuration, watermark setting, Kafka integration, database schema definition, index structure selection, API implementation, and so on. DataSQRL compiles all that for you.\\n\\nDataSQRL also supports inserts to ingest events, subscriptions to push data updates in real-time to the client, and exporting data to Kafka topics or downstream systems. Take a look at the [Quickstart tutorial](/docs/getting-started/quickstart) which shows you how to do that - it only takes a few minutes.\\n\\nDataSQRL is an [open-source project](https://github.com/DataSQRL/sqrl). If you like the idea of DataSQRL, have questions, or need help building your streaming application, [don\u2019t hesitate to reach out](/community).\\n\\nTo sum up, DataSQRL is a tool for simplifying the development of Apache Flink applications by automating the integration of various technologies and allowing developers to focus on their application logic. It makes Flink accessible to lazy developers like us.\\n\\nHave fun building applications with Flink!"},{"id":"sqrl-high-level-data-language-sql","metadata":{"permalink":"/blog/sqrl-high-level-data-language-sql","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-05-22-sqrl-high-level-data-language-sql.md","source":"@site/blog/2023-05-22-sqrl-high-level-data-language-sql.md","title":"SQRL: Enhancing SQL to a High-Level Data Language","description":"When creating data-intensive applications or services, your data logic (i.e. the code that defines how to process the data) gets fragmented across multiple data systems, languages, and mental models. This makes data-driven applications difficult to implement and hard to maintain.","date":"2023-05-22T00:00:00.000Z","formattedDate":"May 22, 2023","tags":[{"label":"SQRL","permalink":"/blog/tags/sqrl"},{"label":"community","permalink":"/blog/tags/community"}],"readingTime":7.505,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"Founder of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"sqrl-high-level-data-language-sql","title":"SQRL: Enhancing SQL to a High-Level Data Language","authors":["matthias"],"tags":["SQRL","community"]},"prevItem":{"title":"Simplifying Apache Flink Application Development with DataSQRL","permalink":"/blog/simplifying-flink-app-development"},"nextItem":{"title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","permalink":"/blog/lets-uplevel-database-datasqrl"}},"content":"When creating data-intensive applications or services, your data logic (i.e. the code that defines how to process the data) gets fragmented across multiple data systems, languages, and mental models. This makes data-driven applications difficult to implement and hard to maintain.\\n\\nSQRL is a high-level data programming language that compiles into executables for all your data systems, so you can implement your data logic in one place. SQRL adds support for data streams and relationships to SQL while maintaining its familiar syntax and semantics.\\n\\n## Why Do We Need SQRL?\\n\\n<img src=\\"/img/reference/reactive_data_layer.svg\\" alt=\\"Data Layer of data-driven application >\\" width=\\"30%\\"/>\\n\\nThe data layer of a data-driven application comprises multiple components: There\u2019s the good ol\u2019 database for data storage and queries, a server for handling incoming data and translating API requests into database queries, a queue/log for asynchronous data processing, and a stream processor for pre-processing and writing new data to the database. Consequently, your data processing code becomes fragmented across various systems, technologies, and languages.\\n\\n\x3c!--truncate--\x3e\\n\\nFor example, consider a project I was once working on. We were building a data service integrating customer data from various silos into a data API for a mobile app. The objective was to provide customers with an integrated view of their service and billing history, support requests, profile information, etc. This is a typical \\"Customer 360\xb0\\" challenge many large organizations face when customer data is spread across numerous operational systems.\\n\\nThe data layer of that service consisted of a bunch of scripts ingesting customer data from CDC (change-data-capture) streams, a SQL database for data persistence, and a Java-based API server.\\n\\nThe data logic of this application was trivial: some translation of billing codes and aggregation of service items, but mostly it was straight-forward mapping of data. Yet, it took the team multiple months to build a prototype because of all the code fragmentation and glue code we had to write to stitch the components together. Integration testing was a big pain. And a simple sprint ticket to add a customer field took a week to implement and test.\\n\\n<img src=\\"/img/blog/tower-of-babel.jpg\\" alt=\\"The Tower of Babel >|\\" width=\\"35%\\"/>\\n\\nThe worst part was all the miscommunication. See, each component of the data layer has a different mental model.\\nFor the data ingestion and pre-processing, the developers thought in terms of events and streams. For the database modeling and querying, the developers thought in terms of rows and tables. And for the API implementation, the developers thought in terms of objects and classes.\\n\\nIt felt like we were building the Tower of Babel. Everybody was speaking a different language and we couldn\u2019t understand each other. But with a twist: We thought we understood each other until it was time to integrate the components and we discovered a mismatch in how we represented the data. That\u2019s a type of punishment not even a jealous God will dish out.\\n\\nTo save ourselves from this tedious work and mental gymnastics, we built SQRL as a high-level data programming language for implementing the data logic of your application in one place.\\n\\n## Introducing SQRL\\n\\nSQRL enhances SQL. If you\u2019ve used SQL before, we hope that you find it easy to pick up SQRL. And if not, there is always ChatGPT \ud83d\ude1c.\\n\\nLet\u2019s take a look at a SQRL script implementing a Customer 360\xb0 API that integrates and aggregates customer and order data:\\n\\n```sql\\nIMPORT datasqrl.seedshop.Orders;\\nIMPORT datasqrl.seedshop.Customers;\\nIMPORT time.*;\\n\\n/* Clean orders data and compute subtotals */\\nOrders.items.discount := discount?0.0;\\nOrders.items.total    := quantity * unit_price - discount;\\nOrders.totals         := SELECT sum(total) as price,\\n                         sum(discount) as saving FROM @.items;\\n/* Deduplicate customer CDC stream */\\nCustomers := DISTINCT Customers ON id ORDER BY timestamp DESC;\\n/* Create relationship between Customers and Orders */\\nCustomers.purchases := JOIN Orders ON Orders.customerid = @.id;\\n/* Aggregate customer spending by state */\\nCustomers.spending := SELECT state, sum(t.price) AS spend,\\n                             sum(t.saving) AS saved\\n                      FROM @.purchases.totals t GROUP BY state;\\n```\\n\\nThis script imports customer data and order streams. It processes data in multiple steps, culminating in an aggregated spending analysis by state.\\n\\nAnd that\u2019s all you have to implement to get a functioning customer 360\xb0 API. DataSQRL compiles this script into executables for all your data systems and handles data mapping and schema synchronization between them.\\n\\n## SQRL Features Overview\\n\\n### Simple Syntax\\n\\nThe first thing you notice is the syntactic sugar that SQRL adds to SQL.\\n\\nIt allows you to define the data sources that you are importing into your script so that a package manager can handle data access configuration and schema management.\\n\\nIt uses the `:=` assignment operator to define new tables and allows incremental column definitions.\\n\\nThe goal is to make SQRL feel a little more like a development language where you build your data logic as a sequence of small, incremental steps instead of writing one massive query.\\n\\n### Nested Data\\n\\nNested data, like JSON documents, is ubiquitous in data-driven applications. It\u2019s how we exchange data. It\u2019s how we expose data in APIs.\\n\\nSQRL provides native support for nested data by representing it as child tables, accessed through the familiar \\".\\" dot notation.\\n\\nIn the example, we sum up the price and saving for all items in an order:\\n```sql\\nOrders.totals := SELECT sum(total) as price, sum(discount) as saving FROM @.items;\\n```\\n\\nThere are a couple of things happening here:\\n\\n- We define a new nested table in `Orders` called `totals` that contains the aggregates\\n- The `FROM` clause `@.items` selects the items from **each** order. The special table handle `@` refers to the parent table in the local context, i.e. `Orders` in this example.\\n\\nBeing able to write queries within a nested context makes it possible to process tree-structured data within SQL.\\n\\nFor example, when we define the `totals` column for each item in an order, we can refer to the other columns of `items` within the local context:\\n\\n```sql\\nOrders.items.total := quantity * unit_price - discount;\\n```\\n\\nNested data support simplifies data consumption from external sources and result data mapping to API calls, eliminating a significant amount of mapping and data transformation code.\\n\\n### Relationships\\n\\nSQRL adds relationships to SQL. You can define relationship columns on tables that relate to rows in other tables using the familiar JOIN syntax.\\n\\n```sql\\nCustomers.purchases := JOIN Orders ON Orders.customerid = @.id;\\n```\\n\\nMaking relationships explicit in SQL simplifies joins and adds structure to the data that is exposed in the API without separate mapping logic.\\n\\nFor example, the `FROM` clause of the spending analysis query uses the relationship expression `@.purchases.totals` to select from the nested `totals` table of the purchase orders for each customer. It eliminates a double-join and makes the query easier to read.\\n\\nSupport for relationships and nested data makes it convenient to handle inter-related data and bridges the gap between the relational data model and the tree or object-relationship structure we use in our APIs and applications.\\n\\n### Stream Processing\\n\\n<img src=\\"/img/blog/data_stream.jpg\\" alt=\\"Matrix Data Stream >|\\" width=\\"40%\\"/>\\n\\nSQRL introduces support for stream tables to ingest external data streams and react to data changes. Data streams are an important part of data-driven applications. It\u2019s how we consume data from other systems or applications and communicate changes in data to subscribers.\\n\\nUnlike normal SQL tables where records can change over time, a stream table has immutable records that are fixed in time. As we saw with the orders stream in our example, SQRL makes it easy to process stream data in steps.\\n\\nSQRL has operators to convert between stream tables and state tables. Our customer 360\xb0 script uses the `DISTINCT` operator to convert a CDC stream into a state table. The `STREAM` operator creates a change stream from a state table, so you can react to changes in state.\\n\\nIn addition, SQRL overloads the `JOIN` operator to support time-consistent joins between state and stream tables. For example, consider the join between the `Customers` and `Orders` tables in the spending analysis query. We want to join the `Orders` stream with the state of the `Customers` table **at the time** of a particular order, so that we aggregate by the state that the customer lived in when the order was placed. If we had used an `INNER JOIN`, the state would update every time the customer moved and the query would aggregate all orders under the state the customer currently lives in.\\n\\nMaking stream tables a first-class citizen in SQL allows us to process stream data, react to changes in data, and bridge the mental model between the set semantics of the relational world and the event orientation of streams.\\n\\nTake a look at the [documentation](/docs/getting-started/intro/sqrl) for a more detailed rundown of all the features SQRL adds to SQL.\\n\\n## Help Us Design SQRL\\n\\nTo take SQRL for a spin and learn how to build data-driven applications, we recommend you start with the [Quickstart tutorial](/docs/getting-started/quickstart). If you have questions, we are happy to answer them on [our Discord](https://discord.gg/49AnhVY2w9).\\n\\nSQRL is still young, and we would love to hear [your feedback](https://discord.gg/49AnhVY2w9) on the language to shape its future."},{"id":"lets-uplevel-database-datasqrl","metadata":{"permalink":"/blog/lets-uplevel-database-datasqrl","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-05-15-lets-uplevel-database-datasqrl.md","source":"@site/blog/2023-05-15-lets-uplevel-database-datasqrl.md","title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","description":"We need to make it easier to build data-driven applications. Databases are great if all your application needs is storing and retrieving data. But if you want to build anything more interesting with data - like serving users recommendations based on the pages they are visiting, detecting fraudulent transactions on your site, or computing real-time features for your machine learning model - you end up building a ton of custom code and infrastructure around the database.","date":"2023-05-15T00:00:00.000Z","formattedDate":"May 15, 2023","tags":[{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"},{"label":"community","permalink":"/blog/tags/community"}],"readingTime":4.79,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"Founder of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"lets-uplevel-database-datasqrl","title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","authors":["matthias"],"tags":["DataSQRL","community"]},"prevItem":{"title":"SQRL: Enhancing SQL to a High-Level Data Language","permalink":"/blog/sqrl-high-level-data-language-sql"},"nextItem":{"title":"DataSQRL 0.1: A SQRL is born","permalink":"/blog/datasqrl-01-release"}},"content":"**We need to make it easier to build data-driven applications.** Databases are great if all your application needs is storing and retrieving data. But if you want to build anything more interesting with data - like serving users recommendations based on the pages they are visiting, detecting fraudulent transactions on your site, or computing real-time features for your machine learning model - you end up building a ton of custom code and infrastructure around the database.\\n\\n<a href=\\"https://www.youtube.com/watch?v=m5uYtBFSmUs&ab_channel=DataSQRL\\" target=\\"_blank\\">\\n<img src=\\"/img/blog/uplevel_play_image.jpg\\" alt=\\"Watch the video version >|\\" width=\\"50%\\"/>\\n</a>\\n\\nYou need a queue like Kafka to hold your events, a stream processor like Flink to process data, a database like Postgres to store and query the result data, and an API layer to tie it all together.\\n\\nAnd that\u2019s just the price of admission. To get a functioning data layer, you need to make sure that all these components talk to each other and that data flows smoothly between them. Schema synchronization, data model tuning, index selection, query batching \u2026 all that fun stuff.\\n\\nThe point is, you need to do a ton of data plumbing if you want to build a data-driven application. All that data plumbing code is time-consuming to develop, hard to maintain, and expensive to operate.\\n\\nWe need to make building with data easier. That\u2019s why we are sending out this call to action to uplevel our database game. **Join us in figuring out how to simplify the data layer.**\\n\\nWe have an idea to get us started: Meet DataSQRL.\\n\\n\x3c!--truncate--\x3e\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"30%\\"/>\\n\\n\\n## Introducing DataSQRL\\n\\nDataSQRL is a build tool that compiles your application\u2019s data layer from a high-level data development language, dubbed SQRL.\\n\\nOur goal is to create a new abstraction layer above the low-level languages often used in data layers, allowing a compiler to handle the tedious tasks of data plumbing, infrastructure assembly, and configuration management.\\n\\nMuch like how you use high-level languages such as Javascript, Python, or Java instead of Assembly for software development, we believe a similar approach should be used for data. \\n\\nSQRL is designed to be a developer-friendly version of SQL, maintaining familiar syntax while adding features necessary for building data-driven applications, like support for nested data and data streams.\\n\\nCheck out this simple SQRL script to build a recommendation engine from clickstream data.\\n\\n```sql\\nIMPORT datasqrl.example.clickstream.Click;  -- Import data\\n/* Find next page visits within 10 minutes */\\nVisitAfter := SELECT b.url AS beforeURL, a.url AS afterURL,\\n                     a.timestamp AS timestamp\\n        FROM Click b JOIN Click a ON b.userid=a.userid AND\\n                b.timestamp <= a.timestamp AND\\n                b.timestamp >= a.timestamp - INTERVAL 10 MINUTE;\\n/* Recommend pages that are visited shortly after */\\nRecommendation := SELECT beforeURL AS url, afterURL AS rec,\\n                         count(1) AS frequency FROM VisitAfter\\n        GROUP BY url, rec ORDER BY url ASC, frequency DESC;\\n```\\n\\nThis little SQRL script imports clickstream data, identifies pairs of URLs visited within a 10-minute interval, and compiles these pairs into a set of recommendations, ordered by the frequency of co-visits.\\n\\n<img src=\\"/img/reference/reactive_data_layer.svg\\" alt=\\"Reactive Data Layer Compiled by DataSQRL >\\" width=\\"30%\\"/>\\n\\nDataSQRL then takes this script and compiles it into an integrated data layer, complete with all necessary data plumbing pre-installed. It configures access to the clickstream. It generates an executable for the stream processor that ingests, validates, joins, and aggregates the clickstream data. It creates the data model and writes the aggregated data to the database. It synchronizes timestamps and schemas between all the components. And it compiles a server executable that queries the database and exposes the computed recommendations through a GraphQL API.\\n\\n**The bottom line: These 9 lines of SQRL code can replace hundreds of lines of complex data plumbing code and save hours of infrastructure setup.**\\n\\nWe believe that all this low-level data plumbing work should be done by a compiler since it is tedious, time-consuming, and error-prone. Let\u2019s uplevel our data game, so we can focus on **what** we are trying to build with data and less on the **how**.\\n\\n## Join Us on this Journey\\n\\n<img src=\\"/img/blog/undraw_collaboration.svg\\" alt=\\"Join DataSQRL Community >\\" width=\\"50%\\"/>\\n\\n\\nWe have the ambitious goal of designing a higher level of abstraction for data to enable millions of developers to build data-driven applications.\\n\\nWe [just released](https://github.com/DataSQRL/sqrl/releases/tag/v0.1.0) the first version of DataSQRL, and we recognize that we are at the beginning of a long, long road. We need your help. If you are a data nerd, like building with data, or wish it was easier, please [join us on this journey](/community). DataSQRL is an open-source project, and all development activity is transparent.\\n\\nHere are some ideas for how you can contribute:\\n\\n* Share your thoughts: Do you have ideas on how we can improve the SQRL language or the DataSQRL compiler? Jump into [our discord](https://discord.gg/49AnhVY2w9) and let us know!\\n* Test the waters: Do you like playing with new technologies? Try out [DataSQRL](/docs/getting-started/quickstart) and let us know if you find any bugs or missing features.\\n* Spread the word: Think DataSQRL has potential? Share this blog post and [star](https://github.com/DataSQRL/sqrl) DataSQRL on [Github](https://github.com/DataSQRL/sqrl). Your support can help us reach more like-minded individuals.\\n* Code with us: Do you enjoy contributing to open-source projects? Dive into [the code](https://github.com/DataSQRL/sqrl) with us and pick up a [ticket](https://github.com/DataSQRL/sqrl/issues).\\n\\nLet\u2019s uplevel our database game. With your help, we can make building with data fun and productive.\\n\\n## More Information\\n\\nYou probably have a ton of questions now. How do I import my own data? How do I customize the API? How do I deploy SQRL scripts to production? How do I import functions from my favorite programming language?\\n\\nThose are all great questions. Check out [datasqrl.com](/) for answers, [join the community](/community) to ask us, or wait for a future blog post where we dive into all of those topics."},{"id":"datasqrl-01-release","metadata":{"permalink":"/blog/datasqrl-01-release","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-05-11-datasqrl-v0.1.md","source":"@site/blog/2023-05-11-datasqrl-v0.1.md","title":"DataSQRL 0.1: A SQRL is born","description":"After two long years of research, development, and teamwork, we\'re excited to announce DataSQRL 0.1! DataSQRL is a tool for building APIs from your data streams and datasets by defining your use case in an SQL.","date":"2023-05-11T00:00:00.000Z","formattedDate":"May 11, 2023","tags":[{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"},{"label":"release","permalink":"/blog/tags/release"}],"readingTime":1.625,"hasTruncateMarker":true,"authors":[{"name":"Daniel Henneberger","title":"Founder of DataSQRL","url":"https://github.com/henneberger","imageURL":"/img/headshots/daniel1.png","key":"daniel"}],"frontMatter":{"slug":"datasqrl-01-release","title":"DataSQRL 0.1: A SQRL is born","authors":["daniel"],"tags":["DataSQRL","release"]},"prevItem":{"title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","permalink":"/blog/lets-uplevel-database-datasqrl"},"nextItem":{"title":"The Two Core Data Problems for Developers: Transactional & Reactive","permalink":"/blog/types-of-data-problems-transactional-reactive"}},"content":"After two long years of research, development, and teamwork, we\'re excited to announce DataSQRL 0.1! [DataSQRL](/) is a tool for building APIs from your data streams and datasets by defining your use case in an SQL.\\n\\n<img src=\\"/img/blog/datasqrlv0.1.jpeg\\" alt=\\"DataSQRL v0.1 release: A SQRL is Born >\\" width=\\"40%\\"/>\\n\\n\\nThis is our first \u201cofficial\u201d release of DataSQRL after many months of testing and bug-fixing. <br />\\nCheck out the [release notes](https://github.com/DataSQRL/sqrl/releases/tag/v0.1.0) on GitHub for a rundown of all the features.\\n\\n\x3c!--truncate--\x3e\\n\\n## Our Vision\\n\\nEvery time we wanted to build a new use case for our application and expose a data API, we found ourselves getting distracted. Distracted by all the orchestration, the technology choices, all the micro-decisions, and the \'plumbing\' that goes into the modern data layer. So we up-leveled the abstraction and kept it simple. We [designed DataSQRL](/docs/getting-started/concepts/why-datasqrl) to handle those nitty-gritty details, so you could stay focused on what truly mattered - building cool things.\\n\\n## Simplicity Through SQL\\n\\nWe\'ve kept [DataSQRL true to SQL](/docs/getting-started/concepts/datasqrl), so it feels familiar and easy to use. We enhanced and modernized the language while maintaining the simplicity of SQL queries. No more wrestling with subqueries, window functions, or repetitive joins - just straightforward SQL.\\n\\n## Flexible APIs\\n\\nOne size doesn\'t fit all when it comes to APIs. We made DataSQRL non-opinionated, giving you the freedom to use your [preferred GraphQL schema](/docs/reference/api/graphql/design) and customize your query patterns with SQRL scripts.\\n\\n## Our Road Ahead\\n\\nWe\'re seeking [your feedback](/community) to help shape the future of DataSQRL. Our current architecture supports a range of platforms, and we\'re working on making it more extensible and useful. Your input is invaluable as we continue to refine and expand DataSQRL\'s capabilities.\\n\\n## Conclusion\\n\\nThe only danger now is that your boss might think he can start coding again. [Join us](/community) as we explore the story behind DataSQRL, its impact on the world of data processing, and the exciting possibilities it holds for the future."},{"id":"types-of-data-problems-transactional-reactive","metadata":{"permalink":"/blog/types-of-data-problems-transactional-reactive","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-05-01-two-types-of-data-problems-transactional-reactive.md","source":"@site/blog/2023-05-01-two-types-of-data-problems-transactional-reactive.md","title":"The Two Core Data Problems for Developers: Transactional & Reactive","description":"Introduction","date":"2023-05-01T00:00:00.000Z","formattedDate":"May 1, 2023","tags":[{"label":"data","permalink":"/blog/tags/data"}],"readingTime":12.825,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"Founder of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"types-of-data-problems-transactional-reactive","title":"The Two Core Data Problems for Developers: Transactional & Reactive","authors":["matthias"],"tags":["data"]},"prevItem":{"title":"DataSQRL 0.1: A SQRL is born","permalink":"/blog/datasqrl-01-release"},"nextItem":{"title":"Hello, World!","permalink":"/blog/welcome"}},"content":"## Introduction\\nEvery developer, whether you build applications or backend services, encounters two distinct types of data problems: transactional and reactive. The need to store and retrieve application state is a quintessential example of a transactional data problem. Conversely, when you\'re processing events or consuming data from external sources, you\'re confronted with a reactive data problem. \\n\\nKnowing which problem you\'re up against is crucial to selecting the right tools from your developer\'s kit. It\u2019s important to determine what type of data problem you are dealing with to choose the right tools and approaches for implementing a solution. After all, using a hammer for a screw job can leave you with more than a few cracks to mend.\\n\\nIn this post, we\'ll guide you on how to differentiate between transactional and reactive data problems and pick the right tools and strategies to solve each of them.\\n\\n\x3c!--truncate--\x3e\\n\\nTable of Contents:\\n* [The Pitfall of Misinterpreting Reactive Problems as Transactional](#pitfall)\\n* [What are Transactional Data Problems?](#what-tx)\\n* [Solving Transactional Data Problems](#solve-tx)\\n* [What are Reactive Data Problems?](#what-rx)\\n* [Solving Reactive Data Problems](#solve-rx)\\n* [Conclusion](#conclusion)\\n\\n## The Pitfall of Misinterpreting Reactive Problems as Transactional {#pitfall}\\n\\n<img src=\\"/img/blog/arrived_logo.png\\" alt=\\"Arrived Logo >\\" width=\\"30%\\"/>\\n\\nLet\'s kick things off with an anecdote from my career. An episode where I mistakenly treated a reactive data problem as a transactional one, resulting in a full-blown application meltdown. Definitely not a shining moment of my career.\\n\\nFlashback to 2011, I was the backend developer for a sprouting startup named \\"Arrived\\". Our vision was to connect people in the real world by using their phone\u2019s location data. That was the time when smartphones started to support geo-fencing and folks thought Foursquare was going to become the next Facebook. Fun times.\\n\\nWe built an iPhone app that allowed users to establish geo-fences and automatically check-in, alerting their connections upon entry. For a brief overview of the app, check out [this brutally hilarious review](https://techcrunch.com/2011/11/10/i-am-a-passenger-and-i-arrive-and-arrived/) from our TechCrunch Disrupt final presentation. While it was soul-crushing at the time, it\'s quite a fun read in retrospect.\\n\\nI implemented the backend of the application as a Java web service, complete with a REST API for user creation, user connection management, and alert dispatch. The API primarily dealt with storing and retrieving user states, connections, geo-fences, and more. These are typical transactional data problems: how to maintain your application state in a durable, reliable, and consistent way. To tackle this, I used MySQL as the database and an object-relational mapping library to translate my Java objects to database rows.\\n\\nAll was sailing smoothly until we decided to implement a social feature to enhance the onboarding experience and boost the app\'s \\"virality\\". This feature uploaded a user\u2019s phone contacts to check if any of their contacts were already using Arrived, suggesting them as potential connections.\\n\\n<img src=\\"/img/blog/arrived_app.png\\" alt=\\"Arrived Mobile App Screenshot >|\\" width=\\"30%\\"/>\\n\\nThat looked like another transactional data problem to me. Or rather, I was oblivious to the existence of other types of data problems and defaulted to it being transactional.\\n\\nThus, I embarked on a path that would eventually lead to disaster. The \\"upload contacts\\" API call I set up did the following: \\n* stored all contact entries in the database, \\n* ran a for-loop to match any entry hashes already in the database, \\n* added a \\"potential connection\\" record to another table in case of a match.\\n\\nTo my credit, the feature worked as intended. I even had a passing test case. But once we launched the feature in production, our database froze.\\n\\nAs it turned out, some users had an expansive social circle with over a thousand contacts. Running a transaction that writes thousands of records and fires off as many read queries on your primary operational database, which also services all your API requests, is a recipe for disaster. Needless to say, the database was not a fan of this idea and promptly crashed.\\n\\nBut my mistake was not a coding error. The code worked fine. The mistake was failing to realize that the \u201ccontacts matching feature\u201d was a reactive data problem, not a transactional one. We were ingesting data from an external source - a user\u2019s contact list - and reacting to it by comparing matches against our existing user base.\\n\\nIn the upcoming sections, we\'ll delve deeper into the differences between transactional and reactive data problems and how to solve them. We\'ll also revisit my reactive data problem and explore how a more informed approach could have saved me from a full-blown, hair-on-fire database crisis.\\n\\nWe will discuss how I could have solved my reactive data problem better and avoided a hair-on-fire database resurrection after we explore transactional and reactive data problems in more detail and how to distinguish between them.\\n\\nAs for \\"Arrived\\", we learned that our most active users were over-vigilant parents monitoring their children, which was not our target audience. Consequently, we had to close shop in less than two years.\\n\\n## What are Transactional Data Problems? {#what-tx}\\n\\nTransactional data problems arise when you need to store and retrieve data concurrently while maintaining consistency. Here, \\"concurrently\\" refers to the simultaneous reading and writing of data by multiple threads or users. The trick is to ensure that data remains consistent throughout this flurry of updates.\\n\\nThere are two forms of inconsistencies we need to avoid. The first relates to upholding application constraints. For instance, if a username is required to be unique, we cannot allow two user records with identical usernames. This would be inconsistent with our application\'s unique username constraint. We may have several such constraints, like \\"account balances can\'t be negative,\\" or \\"each product id in the orders table must correspond to an existing row in the product table.\\"\\n\\nThe second inconsistency type relates to multiple updates triggered by a single request. We want to dodge situations where only some updates are stored. It\'s an all-or-nothing game - we either want all updates to be stored or none at all. For example, a request to transfer $100 from account A to account B requires updating both account balances. If only account A\'s balance is updated while account B\'s update fails, we\'ve got a magical disappearing act of money.\\n\\nEnsuring data consistency while managing concurrent user updates can be quite a challenge. You might encounter scenarios where two users try to register with the same username simultaneously or two users attempt to withdraw from the same account, causing the balance to plummet below zero. Situations like these are why data storage and retrieval often turn into a \\"problem\\" for developers.\\n\\nTransactional data problems typically surface when storing state for applications that multiple users can access concurrently, or when building CRUD APIs.\\n\\n## Solving Transactional Data Problems {#solve-tx}\\n\\nThe panacea for transactional data problems? Databases. Developers harness the power of databases to efficiently handle the concurrency and consistency issues associated with transactional data problems.\\n\\nHowever, databases aren\'t one-size-fits-all. They differ in the types of consistency and concurrency guarantees they offer. If you\'re using any of the popular relational databases (like Postgres, MySQL, SqlServer, Oracle, and Aurora), breathe easy. They\'re likely equipped with all the support you need. For other databases, it\'s worth checking what exactly they support to avoid surprises down the line.\\n\\nAlongside the choice of database, you\'ll also want to equip yourself with a tool that simplifies interactions with the database from your programming language. Wrestling with databases directly can be cumbersome, requiring the use of drivers, query string writing, and data mapping. If you\'re working in an object-oriented programming language, an object-relational mapping layer (or ORM for short) can be your best friend, translating seamlessly between your application and the database. If not, seek out an SDK or database abstraction layer that\'s compatible with your chosen database.\\n\\n## What are Reactive Data Problems? {#what-rx}\\n\\nYou have a Reactive data problem when your data source is outside your application or service\'s control, and you\'re required to respond to the data quickly. Let\u2019s break this down.\\n\\n### Unconstrained Data\\n\\nWhen the data originates from an external source or isn\'t subject to any application constraints, your application does not control the data source. External data sources could include other systems like logs, message queues, files, external databases, or applications. Here, the data pre-exists independently of your control. For instance, a user\'s contact list is an external data source.\\n\\nMoreover, data could be uncontrolled even within your application, provided it\'s free of any constraints. This includes events that occur organically within your application, such as a user clicking a button or visiting a webpage. These events aren\'t within your direct control - they just happen.\\n\\nThis is a stark contrast to transactional data problems, where the key challenge lies in maintaining data consistency amidst concurrent updates.\\n\\n### The Need for Speed\\n\\nAnother characteristic of reactive data problems is the necessity for swift data processing and result generation. This quick reaction is twofold: it must occur shortly after receiving the data, and it involves computational processing of that data.\\n\\nTake the contacts matching feature in Arrived, for instance. The goal was to encourage users to establish connections during the signup process. Consequently, we had to compute the matches within a few seconds - before the user completed the signup and exited the app.\\n\\nHow swift does this reaction need to be to qualify as \\"quick\\"? It varies according to your use case. Customer-facing use cases typically demand reactions within seconds to minutes, tops. For use cases like fraud detection, system automation, or financial transactions, you may need to respond within milliseconds. If the results can wait for hours or even days, it wouldn\'t qualify as quick.\\n\\nThe \\"reaction\\" element primarily involves generating a response to incoming data, which could either serve back to the user or trigger an action. This could mean processing a user\'s shopping cart to suggest other products they might like, analyzing system metrics to detect potential overload, feeding user activity into a machine learning model for a personalized journey, or evaluating if a transaction request is fraudulent. In each instance, we take a piece of data, evaluate it within the application context, and produce a useful response.\\n\\nIn essence, reactive data problems call for quick, efficient responses to one or multiple data sources. The challenges arise from the need to carry out data-intensive computations rapidly, efficiently, and robustly.\\n\\nReactive data problems commonly crop up in use cases such as:\\n* Personalization or recommendation engines\\n* User experience features\\n* Metrics or time-series analysis\\n* Machine learning features\\n* Fraud detection\\n* Cybersecurity and intrusion detection\\n\\n## How do you solve Reactive Data Problems? {#solve-rx}\\n\\nLet\'s circle back to the reactive data problem of the contacts matching feature. My initial solution involved splitting the transaction into several parts, moving some computation to a background thread, fine-tuning the database schema, and writing a hefty amount of SQL. This strategy was time-consuming, fragile, and a nightmare to maintain. A colleague shrugged it off with \u201cI\u2019ve no idea what\u2019s happening here, but I guess it works\u2026\u201d\\n\\nThe problem? When all you have is a hammer, everything looks like a nail. So, I tried hammering that screw into the wall. As expected, it was neither pretty nor productive.\\n\\n<img src=\\"/img/blog/reactive_data_layer_arrived.svg\\" alt=\\"A Reactive Data Layer Architecture >\\" width=\\"40%\\"/>\\n\\nTo solve reactive data problems more effectively, we need to reimagine our data layer architecture. Let\u2019s give our database some company by introducing additional components that make it easier to process data reactively:\\n* **Queue**: Introducing a persisted message queue (or log) into our architecture can ease asynchronous data processing. This robust, scalable tool allows you to write incoming data to the queue and process it when resources are available, significantly simplifying multi-step data processing.\\n* **Stream Processor**: This is a framework dedicated to managing consecutive data processing steps, from data ingestion (reading data off the queue or from external systems) to data transformation, to writing the results to the database. This framework handles all task scheduling and execution, allowing you to focus on the actual processing logic.\\n* **Server**: This component accepts incoming data, writes it to the queue, and serves the processed data from the database. Acting as the entry and exit point for the reactive data layer, the server brings everything together. You can integrate this functionality into an existing API server implementation or create a standalone server to isolate the reactive data use case into its own backend service.\\n\\nTo improve the contact matching feature, we finally adopted a reactive data architecture as illustrated in the diagram above. Here\'s how the data flowed:\\n\\n1. The server received the submitted contacts and wrote the entire data blob to a persisted messaging queue.\\n2. Three asynchronous tasks ran in the processing framework:\\n    1. *Splitter*: This task read an entire contacts list from the queue, divided it into chunks of a maximum of 50 contacts, and wrote the resulting chunks back to the queue under a different topic.\\n    2. *Storer*: This task read contact chunks from the queue and wrote the contact entries as individual records to the database.\\n    3. *Matcher*: This task read contact chunks, matched the contact entries against the user table, and wrote the found matches to the database.\\n3. The database stored contact entries and contact matches in a separate logical database, isolated from the main operational database serving the CRUD API of the Arrived app.\\n4. The server responded to \\"recommended contacts\\" requests during the signup process by running a query against the database that combined all pre-computed matches for the user with matches from checking the user\'s phone number against previously stored contact entries.\\n\\nThis solution was not only efficient and robust but also easier to maintain. Above all, it allowed us to concentrate on enhancing the feature instead of hacking around the database.\\n\\nSo, here\'s the key takeaway: instead of grappling with reactive data problems using a database alone, build a reactive data layer. It will save you considerable time and frustration.\\n\\nIf building a custom data layer seems intimidating, consider checking out DataSQRL. It\'s a tool that constructs reactive data layers for you. We\'ve been developing DataSQRL to assist developers in resolving reactive data problems quickly and efficiently. We would love to hear your feedback!\\n\\n## Conclusion\\n\\n| Type of Data Problem      | Transactional                 | Reactive                                  |\\n|---------------------------|-------------------------------|-------------------------------------------|\\n| Response time expectation | Milliseconds                  | Milliseconds to minutes                   |\\n| Main challenge            | Consistency under concurrency | Quick Reactions                           |\\n| Source of Data            | Maintained by application     | External or events                        |\\n| Consistency Requirements  | Data constraints & atomicity  | Synchronization in time                   |\\n| **Data Layer Solution**   | **Database + ORM**            | **Queue + Processor + Database + Server** |\\n\\nTransactional data problems arise when your application requires concurrent storage and retrieval of consistent data. On the other hand, reactive data problems occur when your application needs to quickly respond to data from external sources or events. Recognizing the distinction between these two types of data issues is crucial to implementing the most effective solution.\\n\\nFor transactional data problems, a data layer comprising a database and an Object-Relational Mapping (ORM) tool is often the best solution. On the contrary, reactive data problems are more efficiently addressed with a data layer that includes a queue, stream processor, database, and server. Understanding these distinctions and applying the appropriate solutions can significantly improve the efficiency and robustness of your data layer."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-01-26-welcome.md","source":"@site/blog/2023-01-26-welcome.md","title":"Hello, World!","description":"\\" width=\\"40%\\"/>","date":"2023-01-26T00:00:00.000Z","formattedDate":"January 26, 2023","tags":[{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":1.76,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"Founder of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"welcome","title":"Hello, World!","authors":["matthias"],"tags":["DataSQRL"]},"prevItem":{"title":"The Two Core Data Problems for Developers: Transactional & Reactive","permalink":"/blog/types-of-data-problems-transactional-reactive"}},"content":"<img src=\\"/img/generic/undraw_launch.svg\\" alt=\\"Launching DataSQRL >\\" width=\\"40%\\"/>\\n\\nWe are excited to launch [DataSQRL](/about) with the mission to help developers and organizations build with data.\\n\\nCollectively, we have spent decades building or helping others build data services. We have seen many struggles, failures, and piles of money being thrown out the window and figured that there must be a better way. We started DataSQRL to find it.\\n\\nWe believe that the technologies used to build data services are too complex and that the engineering processes used to build them are broken. Here is how we plan to fix these issues.\\n\\n\x3c!--truncate--\x3e\\n\\nWe developed [DataSQRL](/) which  compiles a developer-friendly version of SQL into a fully integrated and optimized data pipeline and API server. It takes care of all the laborious plumbing, data massaging, and stitching together of technologies that makes building data services so harrowing. Check out [this short tutorial](/docs/getting-started/quickstart) to see how it works - it only takes a few minutes to build an end-to-end data service.\\n\\nIn addition, we are refining a value-focused process for implementing data services that we have developed over the years while working with development teams and organizations. The basic idea is to apply the same software engineering principles that have proven to be successful to implementing data services. That means you don\'t need a dedicated team of specialists to implement data services and can keep your customers and stakeholders in the feedback loop. [Click here](/docs/process/intro) to learn more about our process.\\n\\nThat\'s our starting point for enabling developers to build successful data services quickly and efficiently. We think we got some good ideas, but have been building data technologies long enough to realize that there is a fine line between innovation and wishful thinking.<br />\\nWe hope that you will join the [DataSQRL community](/community) to share your experience, insights, and opinions to help set us straight.\\n\\nIf you are trying to enable your organization to turn data into valuable data services, consider [working with us](/services) and [get in touch](/contact).\\n\\nWe are excited to be on this journey and hope you will join us. Let\'s build with data together."}]}')}}]);