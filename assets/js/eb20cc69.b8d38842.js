"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1644],{5788:(e,t,a)=>{a.d(t,{Iu:()=>p,yg:()=>g});var n=a(1504);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),d=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=d(e.components);return n.createElement(l.Provider,{value:t},e.children)},y="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),y=d(a),c=r,g=y["".concat(l,".").concat(c)]||y[c]||m[c]||i;return a?n.createElement(g,o(o({ref:t},p),{},{components:a})):n.createElement(g,o({ref:t},p))}));function g(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[y]="string"==typeof e?e:r,o[1]=s;for(var d=2;d<i;d++)o[d]=a[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},9448:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var n=a(5072),r=(a(1504),a(5788));const i={},o="File System",s={unversionedId:"reference/sources/system/file",id:"reference/sources/system/file",title:"File System",description:"The file data system reads and writes data from a file system. That can be a local or attached file system, a cloud file system like AWS S3, HDFS, or URLs.",source:"@site/docs/reference/sources/system/file.md",sourceDirName:"reference/sources/system",slug:"/reference/sources/system/file",permalink:"/docs/reference/sources/system/file",draft:!1,editUrl:"https://github.com/DataSQRL/datasqrl.github.io/edit/main/docs/reference/sources/system/file.md",tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Data System",permalink:"/docs/category/data-system"},next:{title:"Apache Kafka",permalink:"/docs/reference/sources/system/kafka"}},l={},d=[{value:"File System Connector",id:"file-system-connector",level:2},{value:"Supported File Systems",id:"supported-file-systems",level:3},{value:"Data Format",id:"format",level:3},{value:"Compression",id:"compression",level:3},{value:"Data Discovery",id:"data-discovery",level:2},{value:"Data Sink",id:"data-sink",level:2}],p={toc:d},y="wrapper";function m(e){let{components:t,...a}=e;return(0,r.yg)(y,(0,n.c)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"file-system"},"File System"),(0,r.yg)("p",null,"The file data system reads and writes data from a file system. That can be a local or attached file system, a cloud file system like AWS S3, HDFS, or URLs."),(0,r.yg)("h2",{id:"file-system-connector"},"File System Connector"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-json",metastring:'title="system.discovery.table.json"',title:'"system.discovery.table.json"'},'{\n  "type": "source",\n  "connector": {\n    "name": "file",\n    "directoryURI": "~/datasqrl/datasets/mydata",\n    "fileURIs": [],\n    "filenamePattern": "^([^\\\\.]+?)(?:_part.*)?$",\n  }\n}\n')),(0,r.yg)("p",null,"The file connector has the following configuration options:"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Field Name"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Required?"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"directoryURI"),(0,r.yg)("td",{parentName:"tr",align:null},"URI that identifies the directory of the data."),(0,r.yg)("td",{parentName:"tr",align:null},"Yes, unless ",(0,r.yg)("inlineCode",{parentName:"td"},"fileURIs")," is not empty")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"fileURIs"),(0,r.yg)("td",{parentName:"tr",align:null},"List of URIs that identify the files of data. Can only be used as data source."),(0,r.yg)("td",{parentName:"tr",align:null},"Yes, unless ",(0,r.yg)("inlineCode",{parentName:"td"},"directoryURI")," is not empty")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"filenamePattern"),(0,r.yg)("td",{parentName:"tr",align:null},"Regular expression pattern to identify multiple partitions of a file"),(0,r.yg)("td",{parentName:"tr",align:null},"No")))),(0,r.yg)("p",null,"When ",(0,r.yg)("inlineCode",{parentName:"p"},"directoryURI")," is specified, data discovery locates all files in that directory. In addition, data is loaded incrementally as new files are added to the directory when DataSQRL is running."),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"fileURIs")," is an explicit list of files to use as table sources. Use ",(0,r.yg)("inlineCode",{parentName:"p"},"fileURIs")," when reading the directory is not possible, or you want to control exactly which files in a directory are loaded. ",(0,r.yg)("inlineCode",{parentName:"p"},"fileURIs")," can only be used as a data source."),(0,r.yg)("p",null,"The file connector expects either ",(0,r.yg)("inlineCode",{parentName:"p"},"directoryURI")," or ",(0,r.yg)("inlineCode",{parentName:"p"},"fileURIs")," to be configured, but not both."),(0,r.yg)("p",null,"When a dataset is partitioned into multiple files, configure the ",(0,r.yg)("inlineCode",{parentName:"p"},"filenamePattern")," so that DataSQRL can identify all the files that belong to a single table. The default pattern groups assigns files with a ",(0,r.yg)("inlineCode",{parentName:"p"},"_part")," suffix to one table. For example, the files ",(0,r.yg)("inlineCode",{parentName:"p"},"orders_part1.json"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"orders_part2.json"),", and ",(0,r.yg)("inlineCode",{parentName:"p"},"orders_partSomething.json")," are all source files for the ",(0,r.yg)("inlineCode",{parentName:"p"},"Orders")," table. Note, that the filename pattern is applied to the name of the file without data format or compression extension."),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"filenamePattern")," is often used in combination with ",(0,r.yg)("inlineCode",{parentName:"p"},"directoryURI")," to load in new data into a streaming table as files matching the pattern are added to the directory."),(0,r.yg)("h3",{id:"supported-file-systems"},"Supported File Systems"),(0,r.yg)("p",null,'DataSQRL supports both local and remote file systems for reading and writing data. The "scheme" of the ',(0,r.yg)("inlineCode",{parentName:"p"},"directoryURI")," (i.e. the first part of the URI) determines what file system is used."),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Local File System"),": URIs that start with ",(0,r.yg)("inlineCode",{parentName:"li"},"file:")," or a simple path on the local file system."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"S3"),": URIs that start with ",(0,r.yg)("inlineCode",{parentName:"li"},"s3:"),"."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"HDFS"),": URIS that start with ",(0,r.yg)("inlineCode",{parentName:"li"},"hdfs:"),"."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"URL"),": URIs that start with ",(0,r.yg)("inlineCode",{parentName:"li"},"http:")," or ",(0,r.yg)("inlineCode",{parentName:"li"},"https"),". Can only be used as a source.")),(0,r.yg)("h3",{id:"format"},"Data Format"),(0,r.yg)("p",null,"The file system connector supports all ",(0,r.yg)("a",{parentName:"p",href:"/docs/category/data-format"},"data formats"),"."),(0,r.yg)("p",null,"The file system connector supports automatic data format discovery based on the file name extension. For example, a file that ends in ",(0,r.yg)("inlineCode",{parentName:"p"},".json")," is assumed to have the ",(0,r.yg)("a",{parentName:"p",href:"../../format/json"},"JSON data format"),". That means, it is not necessary to configure a data format in the data system configuration."),(0,r.yg)("p",null,"If a data format is configured, automatic data format discovery is disabled and the format is applied to all files in the directory."),(0,r.yg)("h3",{id:"compression"},"Compression"),(0,r.yg)("p",null,"The file system connector supports data compression and determines how to decompress files based on the extension. For example, a file that ends in ",(0,r.yg)("inlineCode",{parentName:"p"},".json.gz")," is decompressed with gzip."),(0,r.yg)("p",null,"The following compression algorithms are supported:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Gzip"),": Extension ",(0,r.yg)("inlineCode",{parentName:"li"},".gz"),", ",(0,r.yg)("inlineCode",{parentName:"li"},".gzip")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Deflate"),": Extension ",(0,r.yg)("inlineCode",{parentName:"li"},".deflate")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Bzip2"),": Extension ",(0,r.yg)("inlineCode",{parentName:"li"},".bz2")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"XZ"),": Extension ",(0,r.yg)("inlineCode",{parentName:"li"},".xz"))),(0,r.yg)("p",null,"DataSQRL expects the compression extension to come after the data format extension for automatic data format discovery."),(0,r.yg)("h2",{id:"data-discovery"},"Data Discovery"),(0,r.yg)("p",null,"Data discovery locates all files in the configured directory (or list of files). It extracts the extensions to determine the ",(0,r.yg)("a",{parentName:"p",href:"#compression"},"compression algorithm")," (if any) and ",(0,r.yg)("a",{parentName:"p",href:"#format"},"data format")," of the file. If a data format is configured, the format extension is ignored."),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"filenamePattern")," regular expression is applied to the filename without extensions and the match for the first ",(0,r.yg)("a",{parentName:"p",href:"https://regexone.com/lesson/capturing_groups"},"capturing group")," is used as the table name."),(0,r.yg)("p",null,"Data discovery reads the data files to determine the schema if a schema is not specified."),(0,r.yg)("p",null,"Data discovery provides an easy shortcut for adding a data source from a directory with having to create a connector configuration: "),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"docker run --rm -v $PWD:/build datasqrl/cmd discover ~/datasqrl/datasets/mydata\n")),(0,r.yg)("p",null,"Pass the path to a local directory or the URI for a remote directory as the first argument to the ",(0,r.yg)("inlineCode",{parentName:"p"},"discover")," command, and it generates the data system configuration with default options before running data discovery."),(0,r.yg)("h2",{id:"data-sink"},"Data Sink"),(0,r.yg)("p",null,"File system can be configured as a data sink by setting the ",(0,r.yg)("inlineCode",{parentName:"p"},"type")," to ",(0,r.yg)("inlineCode",{parentName:"p"},"sink")," or ",(0,r.yg)("inlineCode",{parentName:"p"},"source_and_sink"),". ",(0,r.yg)("inlineCode",{parentName:"p"},"directoryURI")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"format")," are required fields when using the file system as a sink. The exported stream table is written in batches to partitioned files inside the configured directory using the table name as sub-directory."),(0,r.yg)("p",null,"File system supports dynamic table sinks which means you only need to configure the ",(0,r.yg)("inlineCode",{parentName:"p"},"system.discovery.table.json")," file and DataSQRL creates table sinks at compile time. You don't need to run data discovery or configure individual table sinks when using file system as a dynamic sink."))}m.isMDXComponent=!0}}]);