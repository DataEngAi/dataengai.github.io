"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4987],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>m});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)a=o[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var l=r.createContext({}),c=function(e){var t=r.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},u=function(e){var t=c(e.components);return r.createElement(l.Provider,{value:t},e.children)},h="mdxType",p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),h=c(a),d=n,m=h["".concat(l,".").concat(d)]||h[d]||p[d]||o;return a?r.createElement(m,i(i({ref:t},u),{},{components:a})):r.createElement(m,i({ref:t},u))}));function m(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=a.length,i=new Array(o);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[h]="string"==typeof e?e:n,i[1]=s;for(var c=2;c<o;c++)i[c]=a[c];return r.createElement.apply(null,i)}return r.createElement.apply(null,a)}d.displayName="MDXCreateElement"},4592:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>c});var r=a(7462),n=(a(7294),a(3905));const o={slug:"personalized-ai-search",title:"Personalized AI Search with Vector Embeddings for Semantic Profiles",authors:["matthias"],tags:["AI","Kafka","Flink","Postgres","search","personalization","DataSQRL"]},i=void 0,s={permalink:"/blog/personalized-ai-search",editUrl:"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-11-20-personalized-ai-search.md",source:"@site/blog/2023-11-20-personalized-ai-search.md",title:"Personalized AI Search with Vector Embeddings for Semantic Profiles",description:"A common problem in search is ordering large result sets. Consider a user searching for \u201cjacket\u201d on an e-commerce platform. How do we order the large number of results to show the most relevant products first? In other words, what kind of jackets is the user looking for? Suit jackets, sport jackets, winter jackets?",date:"2023-11-20T00:00:00.000Z",formattedDate:"November 20, 2023",tags:[{label:"AI",permalink:"/blog/tags/ai"},{label:"Kafka",permalink:"/blog/tags/kafka"},{label:"Flink",permalink:"/blog/tags/flink"},{label:"Postgres",permalink:"/blog/tags/postgres"},{label:"search",permalink:"/blog/tags/search"},{label:"personalization",permalink:"/blog/tags/personalization"},{label:"DataSQRL",permalink:"/blog/tags/data-sqrl"}],readingTime:9.865,hasTruncateMarker:!0,authors:[{name:"Matthias Broecheler",title:"CEO of DataSQRL",url:"https://github.com/mbroecheler",imageURL:"/img/headshots/matthias1.png",key:"matthias"}],frontMatter:{slug:"personalized-ai-search",title:"Personalized AI Search with Vector Embeddings for Semantic Profiles",authors:["matthias"],tags:["AI","Kafka","Flink","Postgres","search","personalization","DataSQRL"]},nextItem:{title:"Personalized Recommendations for Current23 with Vector Embeddings in Flink and Kafka",permalink:"/blog/recommendations-current23"}},l={authorsImageUrls:[void 0]},c=[{value:"Step 1: Setup",id:"step-1-setup",level:2},{value:"Step 2: API &amp; Architecture",id:"step-2-api--architecture",level:2},{value:"Step 3: Implementation",id:"step-3-implementation",level:2},{value:"Step 4: Running Our Personalized Search",id:"step-4-running-our-personalized-search",level:2},{value:"Step 5: Next Steps",id:"step-5-next-steps",level:3}],u={toc:c},h="wrapper";function p(e){let{components:t,...a}=e;return(0,n.kt)(h,(0,r.Z)({},u,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("head",null,(0,n.kt)("meta",{property:"og:image",content:"/img/blog/ai_personalized_search_robot.png"}),(0,n.kt)("meta",{name:"twitter:image",content:"/img/blog/ai_personalized_search_robot.png"})),(0,n.kt)("p",null,"A common problem in search is ordering large result sets. Consider a user searching for \u201cjacket\u201d on an e-commerce platform. How do we order the large number of results to show the most relevant products first? In other words, what kind of jackets is the user looking for? Suit jackets, sport jackets, winter jackets?"),(0,n.kt)("p",null,"Often, we have the context to infer what kind of jacket a user is looking for based on their interactions on the site. For example, if a user has men\u2019s running shoes in their shopping cart, they are likely looking for men\u2019s sports jackets when they search for \u201cjacket\u201d."),(0,n.kt)("p",null,"At least to a human that seems pretty obvious. Yet, Amazon will return a somewhat random assortment of jackets in this scenario as shown in the screenshot below."),(0,n.kt)("img",{src:"/img/blog/ai_personalized_search_screenshot.png",alt:"Amazon search results for `jacket` |",width:"100%"}),(0,n.kt)("p",null,"To humans the semantic association between \u201crunning shoes\u201d and \u201csport jackets\u201d is natural, but for machines making such associations has been a challenge.\nWith recent advances in large-language models (LLMs) computers can now compute semantic similarities with high accuracy."),(0,n.kt)("p",null,"We are going to use LLMs to compute the semantic context of past user interactions via vector embeddings, aggregate them into a semantic profile, and then use the semantic profile to order search results by their semantic similarity to a user\u2019s profile."),(0,n.kt)("p",null,"In other words, we are going to rank search results by their semantic similarity to the things a user has been browsing. That gives us the context we are missing when the user enters a search query."),(0,n.kt)("p",null,"In this article, you will learn how to build a personalized shopping search with semantic vector embeddings step-by-step. You can apply the techniques in this article to any kind of search where a user can browse and search a collection of items: event search, knowledge bases, content search, etc."),(0,n.kt)("h2",{id:"step-1-setup"},"Step 1: Setup"),(0,n.kt)("p",null,"First, we need to find some realistic product data we can use for our personalized search. We\u2019ll use a recent web scrape of the Flipkart product catalog from ",(0,n.kt)("a",{parentName:"p",href:"https://www.kaggle.com/datasets/aaditshukla/flipkart-fasion-products-dataset/"},"Kaggle"),". Flipkart is India\u2019s largest e-commerce site. We removed incomplete and duplicate records from the dataset."),(0,n.kt)("img",{src:"/img/blog/ai_personalized_search_robot.png",alt:"AI Shopping Assistant >",width:"40%"}),(0,n.kt)("p",null,"Second, we need a vector embedding model. A vector embedding model maps a piece of text to a high-dimensional vector that preserves the semantic meaning of the text in the sense that two pieces of text that are similar map to vectors that are close to each other in space."),(0,n.kt)("p",null,"The embedding model we are using for this tutorial is a quantized version of the ",(0,n.kt)("a",{parentName:"p",href:"https://www.sbert.net/docs/pretrained_models.html"},"all-MiniLM-L6-v2")," pre-trained model. This is an off-the-shelf model for sentence embedding trained on a large corpus. The model is small and fast while delivering good performance. \u201cQuantized\u201d means that the model has been transformed to run efficiently on CPUs."),(0,n.kt)("p",null,"Download ",(0,n.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1a0oc4By3W0QCdjvKObbjqW_9QfHZQNmz/view"},"this zip archive")," for the data and vector model you need to follow the steps in this tutorial. It also contains all the code we\u2019ll be writing."),(0,n.kt)("h2",{id:"step-2-api--architecture"},"Step 2: API & Architecture"),(0,n.kt)("p",null,"We are going to implement the personalized search as a microservice that exposes a GraphQL API. The API has one query endpoint for the personalized search and one mutation endpoint to record the products a user has browsed. The GraphQL schema for our API is shown below:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-graphql",metastring:"title=searchapi.graphqls",title:"searchapi.graphqls"},"type Query {\n ProductSearch(query: String!, userid: String!): [Products!]\n}\n\n\ntype Products {\n id: String!\n title: String!\n description: String!\n brand: String!\n average_rating: Float!\n sub_category: String!\n}\n\n\ntype Mutation {\n ProductView(view: ProductViewInput!): ProductViewed\n}\n\n\ninput ProductViewInput {\n productid: String!\n userid: String!\n}\n\n\ntype ProductViewed {\n _source_time: String!\n userid: String!\n}\n")),(0,n.kt)("p",null,"We use PostgreSQL to implement the search and store the semantic user profiles. PostgreSQL is a relational database that supports efficient full-text search and vector data types."),(0,n.kt)("p",null,"We use Apache Kafka to capture users' product views so that we can process them asynchronously to compute the aggregated semantic user profiles. Kafka is a stream platform to efficiently store and process data streams."),(0,n.kt)("p",null,"We use Apache Flink to ingest the product data, compute vector embeddings, and compute the semantic user profiles. Flink is a stream processor that reliably processes data from streams with exactly-once guarantees."),(0,n.kt)("p",null,"We use these 3 data systems for a reliable, flexible, and scalable architecture. PostgreSQL manages data at rest, Kafka manages data in motion, and Flink processes the data. The architecture diagram shows how the data moves through those systems."),(0,n.kt)("img",{src:"/img/blog/ai_personalized_search_architecture.svg",alt:"Microservice architecture for Personalized AI Search >",width:"60%"}),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"The ",(0,n.kt)("inlineCode",{parentName:"li"},"ProductView")," mutation endpoint gets invoked when a user views a product on the website."),(0,n.kt)("li",{parentName:"ol"},"The event gets recorded in Kafka"),(0,n.kt)("li",{parentName:"ol"},"Flink processes the ",(0,n.kt)("inlineCode",{parentName:"li"},"ProductView")," events and aggregates them into semantic user profiles (more on that below)."),(0,n.kt)("li",{parentName:"ol"},"Flink also ingests product data from the external product catalog, so we have up-to-date product information to search."),(0,n.kt)("li",{parentName:"ol"},"Flink writes the semantic user profiles and product information to PostgreSQL."),(0,n.kt)("li",{parentName:"ol"},"The ",(0,n.kt)("inlineCode",{parentName:"li"},"ProductSearch")," query endpoint gets invoked when a user searches, which is translated to an SQL query against PostgreSQL, and the results are returned to the user.")),(0,n.kt)("p",null,"We could have implemented the entire search microservice with just PostgreSQL and a server. That\u2019s a simpler architecture but has several downsides:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"We have to implement a lot of data processing and failure handling logic in the server to make it work reliably under load."),(0,n.kt)("li",{parentName:"ol"},"Computing vector embedding and aggregating them synchronously can lead to long latencies."),(0,n.kt)("li",{parentName:"ol"},"A spike in traffic to our shopping site would result in a high server load and lots of inserts to PostgreSQL which would degrade search performance.")),(0,n.kt)("p",null,"Using an event-driven architecture as shown above allows us to process the product views and product updates asynchronously, which results in a more robust, reliable, and performant system."),(0,n.kt)("h2",{id:"step-3-implementation"},"Step 3: Implementation"),(0,n.kt)("p",null,"The big downside of our event-driven architecture is that it\u2019s a lot of work to implement all of those components and make them work efficiently together. Luckily, there is a tool that makes this a lot easier: DataSQRL."),(0,n.kt)("p",null,"DataSQRL is an open-source compiler for event-driven microservices. With DataSQRL we only have to implement the processing logic of our search microservice and DataSQRL compiles the entire event-driven architecture for us."),(0,n.kt)("p",null,"Here is how that works. We implement our personalized search engine in a file called ",(0,n.kt)("inlineCode",{parentName:"p"},"aisearch.sqrl")," using SQL. DataSQRL uses an SQL dialect that adds some syntax and features for stream processing. Let\u2019s look at each of the four parts of the script in more detail."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-sql",metastring:"title=aisearch.sqrl",title:"aisearch.sqrl"},"-- ### PART 1: Imports\nIMPORT externaldata.Products; -- Import product information from the catalog\nIMPORT searchapi.ProductView; -- Ingest ProductView events from the API\n-- Import some functions we are going to need for data processing\nIMPORT vector.*;\nIMPORT string.concat;\nIMPORT text.textsearch;\n\n\n-- ### PART 2: Process product data\n-- Compute a semantic vector embedding for each product\nProducts.semantics := onnxEmbed(concat(title, '\\n', description), '/build/embedding/model_quantized.onnx');\n-- Convert the stream of product updates to a state table with the most recent product information\nProducts := DISTINCT Products ON id ORDER BY _ingest_time DESC;\n\n\n-- ### PART 3: Compute semantic profiles from product views\n-- Join in the semantic vector embedding for the product a user viewed\nProductViewVector := SELECT v.userid, p.semantics, v._source_time\nFROM ProductView v TEMPORAL JOIN Products p ON v.productid = p.id;\n-- Aggregate all those vector embeddings into a semantic profile for each user\nUserProfile := SELECT userid, center(semantics) as semanticContext FROM ProductViewVector GROUP BY userid;\n\n\n-- ### PART 4: Personalized Search\n-- Retrieve the products that match the query and order them by semantic similarity to the user's profile\nProductSearch(@query: String, @userid: String) := SELECT p.* FROM\n(SELECT * FROM Products WHERE textsearch(@query, title) > 0) p\nLEFT JOIN (SELECT * FROM UserProfile WHERE userid = @userid) u\nORDER BY coalesce(cosineSimilarity(p.semantics, u.semanticContext),0.0) DESC;\n")),(0,n.kt)("p",null,"Let\u2019s go over the 4 parts of the implementation:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"We import the product data from an external data system. For this tutorial, the product data is imported from a file (included in the archive), but DataSQRL supports multiple data sources. We also import ",(0,n.kt)("inlineCode",{parentName:"li"},"ProductView")," events from the API that are created when a user invokes the corresponding mutation endpoint. Lastly, we import a few functions that we\u2019ll use in our script."),(0,n.kt)("li",{parentName:"ol"},"We preprocess the ",(0,n.kt)("inlineCode",{parentName:"li"},"Products")," data by computing a vector embedding for the title and description of each product using the LLM. Since we are importing a stream of product data, since products may change over time, we deduplicate the stream to get the most recent version."),(0,n.kt)("li",{parentName:"ol"},"We join the ",(0,n.kt)("inlineCode",{parentName:"li"},"ProductView")," events against the ",(0,n.kt)("inlineCode",{parentName:"li"},"Products")," state table to look up the vector embedding for the product. We use a ",(0,n.kt)("a",{parentName:"li",href:"/blog/temporal-join/"},"temporal")," join so that we look up the associated vector at the time of the event. Then we aggregate all the ",(0,n.kt)("inlineCode",{parentName:"li"},"ProductViewVectors")," by ",(0,n.kt)("inlineCode",{parentName:"li"},"userid")," to create the ",(0,n.kt)("inlineCode",{parentName:"li"},"UserProfile"),"."),(0,n.kt)("li",{parentName:"ol"},"To answer a search query, we retrieve the matching products and the vector that represents the semantic profile of a user. We order the results by their semantic similarity to the semantic user profile by computing the cosine similarity between the respective vectors.")),(0,n.kt)("h2",{id:"step-4-running-our-personalized-search"},"Step 4: Running Our Personalized Search"),(0,n.kt)("p",null,"That\u2019s all we have to implement. Let\u2019s run our personalized search and see how it works in practice."),(0,n.kt)("p",null,"First, we compile the SQRL script to a microservice with the command:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"docker run --rm -v $PWD:/build datasqrl/cmd compile aisearch.sqrl searchapi.graphqls --mnt $PWD\n")),(0,n.kt)("p",null,"Make sure you run the command in the folder where you unzipped the archive containing the data."),(0,n.kt)("p",null,"Second, we run the compiled microservice with docker:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"(cd build/deploy; docker compose up)\n")),(0,n.kt)("p",null,"You can access the API in your browser through GraphiQL, which is an IDE for GraphQL APIs, by opening ",(0,n.kt)("a",{parentName:"p",href:"http://localhost:8888/graphiql/"},"http://localhost:8888/graphiql/"),"."),(0,n.kt)("p",null,"Copy-paste the following GraphQL query into the left-hand side and hit the run button to execute the search query for \u201cjackets\u201d. You\u2019ll get a list of jackets that are offered on Flipkart."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-graphql",metastring:"title=Query",title:"Query"},'{\n    ProductSearch(query: "jacket", userid: "myuserid") {\n    title\n    description\n    brand\n  }\n}\n')),(0,n.kt)("p",null,"The search results are not personalized because we don\u2019t have any context for the user \u201cmyuserid\u201d yet. Let\u2019s add some context by having our user visit a product page for running shoes. We do that by running the following mutation query with a payload:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-graphql",metastring:"title=Query",title:"Query"},"mutation AddView($view: ProductViewInput!) {\n  ProductView(view: $view) {\n    _source_time\n  }\n}\n")),(0,n.kt)("p",null,'Mutations require a payload that specifies which product a user visited. Enter the following JSON payload under "Query Variables" in the bottom left of GraphiQL before running the query:'),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-json",metastring:'title="Query Variables"',title:'"Query','Variables"':!0},'{\n  "view": \n  {\n    "productid": "f40b93aa-c3b2-5cf4-97c0-92151279464f",\n    "userid": "myuserid"\n  }\n}\n')),(0,n.kt)("p",null,'Now, if you run the same search query for \u201cjackets\u201d again with the \u201cmyuserid\u201d (click on the history tab to go back to the first query), you should see that the results are tailored to the semantic context of our user. Or try searching for "shorts" for the same user, and you\'ll get athletic and running shorts.'),(0,n.kt)("img",{src:"/img/blog/ai_personalized_search_graphiql.png",alt:"Personalized search results shown in GraphiQL |",width:"100%"}),(0,n.kt)("h3",{id:"step-5-next-steps"},"Step 5: Next Steps"),(0,n.kt)("p",null,"And that\u2019s how you build a personalized search with vector embeddings. Our implementation is only a few lines of code, because DataSQRL generated all the data plumbing for our microservice."),(0,n.kt)("p",null,"We hope you can use this tutorial as a starting point to implement your own personalized search with LLMs. With DataSQRL, you can add additional sources of data, implement custom scoring functions, and fine-tune the search to your particular use case. For extra credit, you could change the aggregation function to prioritize recent product views or fine-tune the LLM for sentence embedding to our dataset. We\u2019ll discuss those improvements in a future tutorial."),(0,n.kt)("p",null,"Check out the ",(0,n.kt)("a",{parentName:"p",href:"/docs/intro/"},"DataSQRL documentation")," for more information and ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/DataSQRL/sqrl"},"star DataSQRL on Github")," to bookmark the open-source project. Good luck building your AI search and reach out to the ",(0,n.kt)("a",{parentName:"p",href:"/community"},"community")," if you have any questions."))}p.isMDXComponent=!0}}]);