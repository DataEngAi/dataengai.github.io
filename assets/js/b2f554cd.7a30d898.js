"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"recommendations-current23","metadata":{"permalink":"/blog/recommendations-current23","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-09-21-recommendations-current23-ai.md","source":"@site/blog/2023-09-21-recommendations-current23-ai.md","title":"Personalized Recommendations for Current23 with Vector Embeddings in Flink and Kafka","description":"Let\u2019s build a personalized recommendation engine using AI as an event-driven microservice with Kafka, Flink, and Postgres. And since Current23 is starting soon, we will use the events of this event-driven conference as our input data (sorry for the pun). You\u2019ll learn how to apply AI techniques to streaming data and what talks you want to attend at the Kafka conference - double win!","date":"2023-09-21T00:00:00.000Z","formattedDate":"September 21, 2023","tags":[{"label":"AI","permalink":"/blog/tags/ai"},{"label":"Kafka","permalink":"/blog/tags/kafka"},{"label":"Flink","permalink":"/blog/tags/flink"},{"label":"Postgres","permalink":"/blog/tags/postgres"},{"label":"microservice","permalink":"/blog/tags/microservice"},{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":10.625,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"recommendations-current23","title":"Personalized Recommendations for Current23 with Vector Embeddings in Flink and Kafka","authors":["matthias"],"tags":["AI","Kafka","Flink","Postgres","microservice","DataSQRL"]},"nextItem":{"title":"To Preprocess or to Query, that\u2019s the Question!","permalink":"/blog/preprocess-or-query"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/recommendationsCurrent23.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/recommendationsCurrent23.png\\" />\\n</head>\\n\\nLet\u2019s build a personalized recommendation engine using AI as an event-driven microservice with Kafka, Flink, and Postgres. And since Current23 is starting soon, we will use the events of this event-driven conference as our input data (sorry for the pun). You\u2019ll learn how to apply AI techniques to streaming data and what talks you want to attend at the Kafka conference - double win!\\n\\nWe will implement the whole microservice in 50 lines of code thanks to the DataSQRL compiler, which eliminates all the data plumbing so we can focus on building.\\n\\n<img src=\\"/img/blog/recommendationsCurrent23.png\\" alt=\\"Build AI Recommendations with DataSQRL >|\\" width=\\"40%\\"/>\\n\\n\x3c!-- (Click on the video to watch the tutorial or read below) --\x3e\\n\\n# What We Will Build\\n\\nWe are going to build a recommendation engine and semantic search that uses AI to provide personalized results for users based on user interactions.\\n\\nLet\u2019s break that down:\\nOur input data is a stream of conference events, namely the talks with title, abstract, speakers, time, and so forth. We consume this data from an external data source.\\n\\nIn addition, our microservice has endpoints to capture which talks a user has liked and what interests a user has expressed. We use those user interactions to create a semantic user profile for personalized recommendations and personalized search results.\\n\\nWe create the semantic user profile through vector embeddings, an AI technique for mapping text to numbers in a way that preserves the content of the text for comparison. It\u2019s a great tool for representing the meaning of text in a computable way. It\'s like mapping addresses (i.e. street, city, zip, country) onto geo-coordinates. It\u2019s hard to compare two addresses, but easy to compute the distance between two geo-coordinates. Vector embeddings do the same thing for natural language text.\\n\\nThose semantic profiles are then used to serve recommendations and personalized search results. \\n\\n\x3c!--truncate--\x3e\\n\\nTo summarize, our microservice will expose the following API (expressed in GraphQL schema):\\n```graphql\\ntype Mutation {\\n    Likes(liked: LikedInput!): LikeAdded\\n    AddInterest(interest: AddInterest!): InterestAdded\\n}\\n\\ntype Query {\\n    Events(limit: Int!, offset: Int = 0): [Events!]!\\n    EventsLiked(userid: String!): [EventsLiked!]!\\n    RecommendedEvents(userid: String!): [RecommendedEvents!]\\n    PersonalizedEventSearch(query: String!, userid: String!): [PersonalizedEventSearch!]\\n}\\n```\\nThe API has two mutations (for REST folks, think of those as POST): one captures which events a user has liked, and another captures a user\u2019s interests.\\n\\nWe have four query endpoints (those are like GET): two boring ones that return all the events and the events a user has liked and two AI-powered ones that return recommended events for a user and personalized search results for a user\u2019s search query.\\n\\nYou can see the full GraphQL API with the mutation and return types [here](https://gist.github.com/mbroecheler/bd3ba8a8307fc36836a91599b9ff2643).\\n\\n#  Architecture\\n\\nWe will implement our conference recommendation service as an event-driven microservice for robust, real-time processing at scale. The architecture of the microservice is shown below and uses Kafka for event storage, Flink for stream processing, Postgres for querying, and GraphQL for the API.\\n\\n<img src=\\"/img/blog/current23_microservice.svg\\" alt=\\"Event-driven microservice architecture >\\" width=\\"60%\\"/>\\n\\nThe data travels counter-clockwise from the top:\\n\\n1. When a user interaction is captured through the mutation endpoint,\\n2. the input data is written to Kafka as an event, \\n3. which gets picked up by Flink, processed, embedded as a vector, and aggregated into a semantic user profile, \\n4. which is stored in Postgres. \\n5. When a user requests data through the query endpoint, the data is fetched from Postgres. \\n6. In addition, the conference events are ingested by Flink from an external data source and similarly processed and stored in the database for querying.\\n\\nEach component serves a distinct purpose in this event-driven architecture: the GraphQL server acts as the interface to the outside world, Kafka manages events in motion, Flink processes the event stream, and Postgres stores the processed data for retrieval on request.\\n\\n# Implementation\\n\\nNow, we could implement this event-driven microservice by implementing each of the components: implement the GraphQL server, set up the Kafka topics and event schemas, implement a Flink job for data processing, and design a database schema plus SQL queries. We would have to write a ton of data plumbing code: moving data between systems, mapping schemas, designing data models, and stitching it all together. There is a reason event-driven microservices are built by teams of specialists.\\n\\nThere is a better way: We are going to use the open-source DataSQRL compiler to generate all of that data plumbing code for us. That means we can implement our entire microservice in just 50 lines of SQL code as follows ([click here](https://gist.github.com/mbroecheler/315f99fc53768f579014ab9be7cc2fd4) to see the entire SQL script):\\n\\n## Imports\\n\\n```sql\\nIMPORTS\\n```\\n\\nWe import the source tables that we are processing in this script. DataSQRL uses packages to represent data sources for modularity and ease of reuse. It\u2019s like importing an external library but for data.\\nOur API is treated as a data source which allows us to import the mutation input data as a table.\\n\\nWe are also importing functions for string processing, vector embedding, etc. DataSQRL uses the same packaging structure to organize functions.\\n\\nNow, onto the actual logic of our script.\\n\\n## Processing Event Data\\n\\n```sql\\nEvents.id := coalesce(CAST(regexExtract(url, \'(\\\\d*)$\') AS INT),0);\\nEvents.full_text := concat(title,\'\\\\n\',abstract);\\nEvents.startTime := concat(trim(regexExtract(date, \'^[^-]*\')),\' \',trim(regexExtract(time, \'\\\\d\\\\d?:\\\\d\\\\d\\\\s(AM|PM)\')));\\nEvents.startTimestamp := parseTimestamp(concat(startTime,\' PDT\'), \'MMMM d, yyyy h:mm a z\')\\nEvents.embedding := onnxEmbed(full_text, \'/build/embedding/model_quantized.onnx\');\\n\\nEvents := DISTINCT Events ON id ORDER BY last_updated DESC;\\n```\\n\\nFirst, we are adding additional columns to the `Events` table and then deduplicating the data stream so we have the most recent version of each event.\\n\\nWe are adding columns mostly to clean up our ingested events data. When you are dealing with external data, cleanup is often necessary. In this case, we need to do some work to extract the event timestamp and id.\\n\\nWe are also adding the `embedding` column to compute a vector embedding for the `title` and `abstract` of each talk. We are using the [ONNX AI](https://onnxruntime.ai/) runtime to execute the embedding model. The embedding model we are using here is a quantized version of the [all-MiniLM-L6-v2](https://www.sbert.net/docs/pretrained_models.html) pre-trained model. This is a model for sentence embedding trained on a large corpus that is small and fast while delivering good performance. \u201cQuantized\u201d means that the model has been transformed to run efficiently on CPUs for those of us who aren\u2019t hoarding GPUs right now.\\n\\n## Processing User Interactions\\n\\n```sql\\nAddInterest.embedding := onnxEmbed(text, \'/build/embedding/model_quantized.onnx\');\\n\\nLikeVector := SELECT l.userid, e.embedding, l._source_time\\n              FROM Likes l TEMPORAL JOIN Events e ON l.eventId = e.id WHERE l.liked;\\n\\nUserInterestVectors := \\n    (SELECT userid, embedding, _source_time FROM LikeVector)\\n        UNION ALL\\n    (SELECT userid, embedding, _source_time FROM AddInterest)\\n\\nUserInterests := SELECT userid, center(embedding) as interestVector \\n                 FROM UserInterestVectors GROUP BY userid;\\n```\\n\\nNext, we are processing the user interactions. We are adding an embedding vector for the user interests captured in the AddInterst table. We are joining the user `Likes` with the deduplicated events table using a [temporal join](../temporal-join) to get the embedding vector for the liked event. Both of those capture the semantic interests of a user.\\n\\nThen, we combine those data streams in the `UserInterstVectors` table and aggregate them by computing the centroid for all those vectors for each user. That summary of user interest vectors gives us the semantic profile of each user.\\n\\n## User Analytics\\n\\n```sql\\nUserLikes := DISTINCT Likes ON userid, eventId ORDER BY _source_time DESC;\\n\\nEventLikeCount := SELECT eventid, count(*) as num, avg(eventid) as test\\n                  FROM UserLikes l WHERE l.liked GROUP BY eventid;\\n\\nEvents.likeCount := JOIN EventLikeCount l ON @.id = l.eventid;\\n\\nEventsLiked(@userid: String) := \\n    SELECT e.* FROM UserLikes l JOIN Events e ON l.eventId = e.id\\n    WHERE l.userid = @userid\\n    ORDER BY e.startTimestamp ASC;\\n```\\n\\nTo show you that DataSQRL also supports good old-fashioned data analytics, we are adding some user likes analytics. We deduplicate the stream of `Likes` (in case a user liked and then unliked an event) and aggregate them by event into the `EventLikeCount` table.\\n\\nWe add a relationship between `Events` and `EventLikeCount` so that the like count can be accessed from the event through the API. DataSQRL adds relationships to SQL, so you can structure your data for API access and to simplify join expressions.\\n\\nWe add a table function that returns all the events a user has liked which maps onto the query endpoint in the GraphQL API of the same name.\\n\\n## Personalized Recommendation\\n\\n```sql\\nRecommendedEvents(@userid: String) :=\\n    SELECT e.*, cosineSimilarity(i.interestVector, e.embedding) as score\\n    FROM UserInterests i JOIN Events e\\n    WHERE i.userid = @userid ORDER BY score DESC;\\n```\\n\\nTo serve users personalized recommendations, we compute the similarity between the event embedding and the aggregated semantic user profile of the `UserInterests` table using cosine similarity between the vectors.\\n\\n## Personalized Search\\n\\n```sql\\nPersonalizedEventSearch(@query: String, @userid: String) :=\\n    SELECT e.*, coalesce(cosineSimilarity(i.interestVector, e.embedding),0.0) as score\\n    FROM Events e LEFT JOIN UserInterests i ON i.userid = @userid\\n    WHERE textsearch(@query, title, abstract) > 0\\n    ORDER BY score DESC;\\n```\\n\\nFor personalized search, we retrieve those events where the title or abstract matches the search query and then rank the results based on how similar the event is to the aggregated user interests.\\n\\n# Conclusion\\n\\nAnd that\u2019s it. A complete event-driven microservice with vector embedding, personalized search, and user interaction analytics in 50 lines of SQL code.\\n\\nAnd DataSQRL handles all the rest: mapping mutations onto Kafka topics and events, ingesting those events into Flink and mapping schemas, designing the physical data models in Kafka, Flink, and the database, mapping API calls onto database queries, optimizing index structures, and moving the data efficiently between all those components. That\u2019s a whole lot of soul-sucking work we did not have to do.\\n\\nIf you want to learn more about DataSQRL, visit [datasqrl.com](/), take a look at the [in-depth tutorial](/docs/getting-started/intro/overview/), or [join the community](/community) on [Discord](https://discord.gg/49AnhVY2w9) to ask questions and share your thoughts and feedback.\\n\\n# Run Microservice\\n\\nWant to run the recommendation microservice yourself? It\u2019s easy. Follow these steps:\\n\\n1. In your command line, create an empty folder and go into the folder:\\n```bash\\nmkdir current23; cd current23\\n```\\n2. Download the SQRL script, GraphQL schema, vector embedding model and event data source by [clicking here](https://drive.google.com/file/d/15p6erQnG8S4eDVgjxiOdYEfvqdEgjc0M/view?usp=sharing), moving the zip file into folder you just created and unpacking it. You should see 3 directories (conference, conferencedata, and embedding) as well as a `sqrl` and `graphqls` file. The SQRL script and GraphQL schema are the ones we walked through above.\\n\\n3. Compile the SQRL script and GraphQL schema into an event-driven microservice by running:\\n```bash\\ndocker run --rm -v $PWD:/build datasqrl/cmd:dev compile conference-recommendation.sqrl recAPI.graphqls --mnt $PWD\\n```\\n\\n:::info\\nDisclaimer: We are using the preview release of DataSQRL to showcase the vector embedding feature that\'s coming in the next release.\\nUse the latest tag for the stable release.\\n:::\\n\\n\\n4. Stand up the entire microservice in docker by running:\\n```bash\\n(cd build/deploy; docker compose up)\\n```\\n\\n\\nNote, that the microservice does not contain DataSQRL. DataSQRL is only the compiler and generates the docker-compose script for orchestrating the microservice. The microservice itself only consists of Kafka, Flink, Postgres, and GraphQL server.\\nIf you want to take a look at the deployment artifacts that DataSQRL compiled for each component, take a peek inside the `build/deploy` folder.\\n\\nOnce the microservice is up and running (it takes a little while for all the components to boot up and initialize), the GraphQL API is accessible. You can access the API directly or open [http://localhost:8888/graphiql/](http://localhost:8888/graphiql/) to try out queries in your browser.\\n\\nFor example, run this query to get a list of events.\\n\\n```graphql\\n{\\n    Events(limit:20) {\\n        id\\n        title\\n        abstract\\n        time\\n        location\\n    }\\n}\\n```\\n\\nAdd a user interest by running the following mutation:\\n\\n```graphql\\nmutation AddInterest($interest: AddInterest!) {\\n  AddInterest(interest: $interest) {\\n  _source_time\\n}}\\n```\\n\\nAnd add the following query payload under \\"Query Variables\\":\\n```json\\n{\\n  \\"interest\\": {\\n    \\"userid\\": \\"1234\\",\\n    \\"text\\": \\"I want to learn more about Apache Flink and how to use it for real-time stream processing.\\"\\n  }\\n}\\n```\\n\\nThen look at the recommendations for the user `1234`:\\n```graphql\\n{\\n  RecommendedEvents(userid: \\"1234\\") {\\n    id\\n    title\\n    abstract\\n  }\\n}\\n```\\n\\nYou can like an event with this mutation:\\n```graphql\\nmutation AddLike($liked: LikedInput!) {\\n  Likes(liked: $liked) {\\n  _source_time\\n}}\\n```\\nand this payload:\\n```json\\n{\\n  \\"liked\\": {\\n    \\"userid\\": \\"1234\\",\\n    \\"eventId\\": 1136822,\\n    \\"liked\\": true\\n  }\\n}\\n```\\n\\nAnd then see how that impacts the personalized search results with this query:\\n```graphql\\n{\\n  PersonalizedEventSearch(query: \\"kafka\\", userid: \\"1234\\") {\\n    id\\n    title\\n    abstract\\n  }\\n}\\n```\\nAs you can see, our search results are strongly biased in the direction of Apache Flink since that\'s what we liked a Flink talk and expressed an interest in Flink.\\n\\nEnjoy playing with the API and finding the talks you want to attend at the conference.\\n\\nTo shut the microservice down, hit CTRL-C and then run `(cd build/deploy; docker compose down -v)` to remove the volumes."},{"id":"preprocess-or-query","metadata":{"permalink":"/blog/preprocess-or-query","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-08-15-preprocess-or-query.md","source":"@site/blog/2023-08-15-preprocess-or-query.md","title":"To Preprocess or to Query, that\u2019s the Question!","description":"When developing streaming applications or event-driven microservices, you face the decision of whether to preprocess data transformations in the stream engine or execute them as queries against the database at request time. The choice impacts your application\u2019s performance, behavior, and cost. An incorrect decision results in unnecessary work and potential application failure.","date":"2023-08-15T00:00:00.000Z","formattedDate":"August 15, 2023","tags":[{"label":"Kafka","permalink":"/blog/tags/kafka"},{"label":"Flink","permalink":"/blog/tags/flink"},{"label":"Postgres","permalink":"/blog/tags/postgres"},{"label":"microservice","permalink":"/blog/tags/microservice"},{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":13.65,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"preprocess-or-query","title":"To Preprocess or to Query, that\u2019s the Question!","authors":["matthias"],"tags":["Kafka","Flink","Postgres","microservice","DataSQRL"]},"prevItem":{"title":"Personalized Recommendations for Current23 with Vector Embeddings in Flink and Kafka","permalink":"/blog/recommendations-current23"},"nextItem":{"title":"Why Temporal Join is Stream Processing\u2019s Superpower","permalink":"/blog/temporal-join"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/blog/preprocessOrQuery.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/blog/preprocessOrQuery.png\\" />\\n</head>\\n\\nWhen developing streaming applications or event-driven microservices, you face the decision of whether to preprocess data transformations in the stream engine or execute them as queries against the database at request time. The choice impacts your application\u2019s performance, behavior, and cost. An incorrect decision results in unnecessary work and potential application failure.\\n\\n<img src=\\"/img/blog/preprocessOrQuery.png\\" alt=\\"To preprocess or to query? >|\\" width=\\"50%\\"/>\\n\\nIn this article, we\u2019ll delve into the tradeoff between preprocessing and querying, guiding you to make the right decision. We\u2019ll also introduce tools to simplify this process. Plus, you\u2019ll learn how building streaming applications is related to fine cuisine. It\u2019ll be a fun journey through the land of stream processing and database querying. Let\u2019s go!\\n\\n## Recap: Anatomy of a Streaming Application\\n\\nIf you\'re in the process of building an event-driven microservice or streaming application, let\'s recap what that entails. An event-driven microservice consumes data from one or multiple data streams, processes the data, writes the results to a data store, and exposes the final data through an API for external users to access.\\n\\nThe figure below visualizes the high-level architecture of a streaming application and its components: data streams (e.g. Kafka), stream processor (e.g. Flink), database (e.g. Postgres), and API server (e.g. GraphQL server).\\n\\n<img src=\\"/img/blog/dataflow-stages.svg\\" alt=\\"Streaming Application Architecture\\" width=\\"100%\\"/>\\n\\n\\nAn actual event-driven microservice might have a more intricate architecture, but it will always include these four elements: a system for managing data streams, an engine for processing streaming data, a place to store the results, and a server to expose the service endpoint.\\n\\nThis means an event-driven architecture has two stages: the preprocess stage, which processes data as it streams in, and the query stage which processes user requests against the API. Each stage handles data, but they differ in what triggers the processing: incoming data triggers the preprocess stage, while user requests trigger the query stage. The preprocess stage handles data before the user needs it, and the query stage handles data when the user explicitly requests it.\\n\\nUnderstanding these two stages is vital for the successful implementation of event-driven microservices. Unlike most web services with only a query stage or data pipelines with only a preprocess stage, event-driven microservices require a combination of both stages.\\n\\nThis leads to the question: Where should data transformations be processed? In the preprocessing stage or the query stage? And what\u2019s the difference, anyways? That\u2019s what we will be investigating in this article.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Balancing Act: Preprocess vs. Query\\n\\nPicture yourself as the chef of a renowned Italian restaurant famous for its pasta dishes. You must decide what food to prepare during the day and what to cook when the order comes in. Cooking everything from scratch upon receiving an order would lead to long waiting times and require more kitchen staff. However, preparing all dishes ahead of time would result in overwork, significant food waste, and stale food. The solution lies somewhere in between. So you marinate the chicken and harvest the basil in the morning but cook the pasta on order.\\n\\nBuilding streaming applications is a lot like being a chef. You must determine what data to preprocess for immediate availability and what data to compute fresh upon user request. If all your data processing happens in the database or server at request time, you\'ll face high request latencies and need to store a significant amount of data, increasing your costs. However, preprocessing everything might also be too expensive, result in unnecessary computations, and yield stale results.\\n\\nTherefore, the optimal solution for data processing in streaming applications, like in a kitchen, often involves a balance between preprocessing and querying\\n\\n## Striking a Balance: Preprocess or Query?\\n\\nHow do you find the right balance to determine whether a particular data transformation should be preprocessed or queried?\\n\\nThe answer lies in the following 4 requirements of your streaming application:\\n* Latency: What is the maximum request latency for the API endpoint?\\n* Cost: What\u2019s the operational budget for your application?\\n* Freshness: How quickly should incoming data points be reflected in request responses?\\n* Consistency: What API results need to be consistent with each other?\\n\\nLet\u2019s look at each of these factors in detail to learn how they influence the decision on where the data should be processed.\\n\\n### Latency\\n\\nQuantifying the latency requirements of your microservice API is typically straightforward.  For customer-facing microservices, it\u2019s common to expect a p99.9 latency of a few hundred milliseconds, meaning 99.9% of all user requests complete within this time frame. If unsure, your product manager probably has an opinion on the target request latency \ud83d\ude09.\\n\\nThe maximum request latency sets a limit on the amount of data processing you can perform in the query stage. More data processing in the query stage results in longer request latency. This is largely influenced by the number of records you need to process and the complexity of the data processing required.\\n\\nFor instance, if a running deep-learning inference model at request time consumes 700 of your 1000 ms p99.9 latency, you must do most of the data processing (i.e., feature computation) in the preprocessing stage.\\n\\nLikewise, pulling a million records from the database for aggregation is likely going to eat up your entire request latency budget. In particular, watch out for data transformations on data with highly skewed distributions. This occurs when the average number of records processed per request is small, but occasionally you have to process a lot of records which impacts your p99.9 latency.\\n\\n**The general rule of thumb is: The lower the request latency, the more preprocessing you want to do.**\\n\\n### Cost\\n\\nThe cost of running your microservice is another factor you have to keep within a certain budget. Let\'s explore how to optimize the cost of our streaming application, considering both the query and preprocessing stages.\\n\\n#### Query Cost\\n\\nA big factor in our database cost is how much data we need to store. Naturally, that determines the storage cost for the database but it also increases the cost of all database operations. The more data a database must manage, the more computing resources it must utilize to retrieve data, maintain index structures, manage buffers, etc.\\nTherefore, the simplest way to decrease our database expense is by reducing the volume of data we store in it. One effective method to achieve this is by aggregating data in the preprocessing stage.\\n\\nFor example, imagine we are building an IoT microservice that gathers temperature readings from a million sensors that operate at 10Hz (i.e. they take 10 readings every second). This amounts to a quarter gigabyte per second or over 20 TB per day. If we stored all this data in a database, we\u2019d face a pretty hefty bill. However, if we only require min, max, avg, and median readings for each minute, we can aggregate the data during preprocessing and significantly reduce the volume of data written to the database (approximately 500 times less), thereby reducing our database expenses.\\n\\nThe other factor that drives our query stage cost is the amount of data processing. Many applications are read-heavy, meaning the same processed data is accessed multiple times in different requests. By preprocessing that data and storing the result in the database, we can reduce our cost because we only have to run the computation once. This approach is somewhat akin to caching but with the assurance that the results are instantly updated as new data is received by the preprocessing stage.\\n\\n\\n#### Preprocessing Cost\\n\\nNow, let\u2019s look at the preprocessing stage. Similar to the query stage, the cost is driven primarily by the amount of computation and the volume of data that needs to be stored. Stream engines are highly efficient at executing time-bound operations, such as time-window aggregations and temporal joins, because those limit the amount of state - and therefore storage - that the stream engine has to maintain over time.\\n\\nOn the other hand, data transformations that aren\u2019t bound in time can accumulate state indefinitely which is costly, hurts performance, and can crash the entire application due to memory exhaustion. A common culprit for high preprocessing cost is the inner join. Computing inner joins on changing streams requires that the stream engine hold the entire dataset for both sides of the stream in memory AND issue updates for all previously joined records whenever a record on either side of the join changes. That\u2019s why joins are typically much cheaper to execute in the database at request time.\\n\\n**The general rule of thumb is: Reduce the cost of the query stage by preprocessing data but be mindful of costly streaming operations like joins.**\\n\\n### Data Freshness\\n\\nMuch like chicken cooked the previous day, some data preprocessing leads to stale results that aren\u2019t acceptable to the user of your API.\\n\\nFor example, let\u2019s go back to our IoT microservice that collects temperature data. The sensors are producing 10 readings every second that we are aggregating into 1 second time-windows. In other words, we are effectively downsampling our readings from 10 to 1 per second. For applications where users expect to see the most up-to-date temperature that may not be acceptable.\\n\\nThe impact of data freshness on your application is harder to quantify than latency or cost, because it depends a lot on your customer\u2019s expectations and what they are doing with the data you produce. But like the kitchen, you get the freshest results when you prepare everything from scratch when the request comes in.\\n\\n**The general rule of thumb is: The fresher you want your aggregations to be, the more you have to compute at query time.**\\n\\n### Consistency\\n\\nData transformations often utilize a single record of incoming data in multiple ways. Take for example a banking application that presents a list of transactions, categorized by type, and also offers a monthly spending analysis, broken down by type. If a user modifies the type of a transaction, the spending analysis should automatically update. This is what we mean by data consistency: any changes in the input data should be reflected in all the results computed from that data.\\n\\nWhen we preprocess data, such changes are often not immediately reflected or we cannot guarantee that they are reflected in all preprocessed results at the same time. That can lead to inconsistencies in the data returned by the API. A user may update the transaction type but the spending analysis won\u2019t update until a moment later when the preprocessing stage catches up and adjusts the aggregates.\\n\\nLike data freshness, the impact of data inconsistency on your application can be harder to quantify. It typically needs to be evaluated in the context of the entire application, not just the event-driven microservice. For instance, if the front-end application that allows the user to modify the transaction type automatically updates the affected aggregates, the temporary inconsistency of data in the API response may not be problematic.\\n\\nMaintaining data consistency is easier when data computations are executed as queries against the database because we compute the results from the original records.\\n\\n**The general rule of thumb is: The more consistent you want computed results to be across changes in input data, the more computation has to be done in the query stage.**\\n\\n## Practical Advice for Balancing Preprocessing and Querying\\n\\nChoosing between preprocessing and querying isn\'t a straightforward decision. Rather, it\'s like a tug-of-war between your application\'s various requirements that will dictate whether a particular data transformation should be preprocessed or executed at query time. Low latency and cost-effectiveness tend to favor preprocessing, while data freshness and consistency often lean towards querying.\\n\\n**What makes this decision so challenging is that preprocessing data and querying data are implemented very differently.** Preprocessing data with tools like Kafka and Flink requires a different approach, language, and implementation compared to executing the exact same data transformation with Postgres and an API server at query time. It\'s common to have different team members handling preprocessing and query stage implementations. Therefore, moving a single transformation from the query to the preprocess stage (or vice versa) is expensive because it requires reimplementing the transformation, coordination within the team, and refactoring the interface between the components of the microservice.\\n\\nBecause it is so expensive to change what stage we execute our data transformations in, we try to get it right during the planning phase. But we often won\u2019t know what the right balance is until we try it. And, application requirements change over time, of course. That makes event-driven microservice implementations time-consuming and costly.\\n\\nBut what if we could implement all of our data transformations in one script and have a compiler translate them to preprocess stage or query stage implementations based on application requirements? That would eliminate the problem because we can easily change where data transformations get executed. We could try out different allocations and iterate quickly as requirements change.\\n\\n## Meet DataSQRL: The Compiler for Event-Driven Microservices\\n\\nThat\u2019s exactly what [DataSQRL](https://www.datasqrl.com/) does. With DataSQRL, you can write your streaming application\'s data transformations in SQL. The compiler then translates your SQL into preprocess and query implementations. Specifically, DataSQRL translates SQL into a combination of FlinkSQL and the datastream API for preprocessing, and PostgreSQL statements for querying.\\n\\nLet\'s see how this works using our IoT microservice example:\\n\\n```sql\\nIMPORT datasqrl.example.sensors.SensorReading; -- Import sensor data\\nIMPORT time.endOfSecond;  -- Import time function\\n/* Aggregate sensor readings to second */\\nSecReading := SELECT sensorid, endOfSecond(time) as timeSec,\\n                     avg(temperature) as temp FROM SensorReading\\n              GROUP BY sensorid, timeSec;\\n/* Get max temperature in last minute */\\nSensorMaxTemp := SELECT sensorid, max(temp) as maxTemp\\n                 FROM SecReading\\n                 WHERE timeSec >= now() - INTERVAL 1 MINUTE\\n                 GROUP BY sensorid;\\n```\\n\\nFirst, we import the sensor reading data stream from Kafka. We define a new table `SecReading` that aggregates the readings per second. Finally, we create `SensorMaxTemp`, a table that calculates the maximum temperature for each sensor over the last minute.\\n\\nDataSQRL\'s intelligent optimizer automatically determines where to execute the data transformations defined in your SQL script, based on your microservice\'s API.\\nFor our example, we expose the following GraphQL API that allows users to query the `SecReading` and `SensorMaxTemp` tables by `sensorid`:\\n\\n```graphql\\ntype Query {\\n    SecReading(sensorid: Int!): [SecReading!]\\n    SensorMaxTemp(sensorid: Int): [SensorMaxTemp!]\\n}\\n\\ntype SecReading {\\n    sensorid: Int!\\n    timeSec: String!\\n    temp: Float!\\n}\\n\\ntype SensorMaxTemp {\\n    sensorid: Int!\\n    maxTemp: Float!\\n}\\n```\\n\\nBased on the API definition, DataSQRL\'s optimizer finds the best allocation of data transformations to stages by minimizing the request latency and cost. In our example, this means that the `SecReading` table is preprocessed as a tumbling time-window, and `SensorMaxTemp` is preprocessed as a sliding time-window. Both tables are aggregated in Flink during preprocessing, with the results written to Postgres for querying on request.\\n\\nBut what if data freshness of the `SensorMaxTemp` table is more important for our application than low latency? No problem, simply tell the optimizer to execute this data transformation in the database:\\n\\n```sql\\n/*+ EXEC(database) */\\nSensorMaxTemp := SELECT sensorid, max(temp) as maxTemp\\n                 FROM SecReading\\n                 WHERE timeSec >= now() - INTERVAL 1 MINUTE\\n                 GROUP BY sensorid;\\n```\\n\\nWith a simple hint above the table definition, you can direct the optimizer where to process the table, and it compiles the microservice accordingly. What would have required a significant refactor of your entire microservice is now a simple code change.\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"30%\\"/>\\n\\nDataSQRL also handles all the data plumbing needed to integrate your event-driven microservice components. There\'s no need to connect Flink to Kafka, define the Flink table schema, define the Postgres table schema, or map API requests to prepared SQL statements. The DataSQRL compiler does it all for you. You get the same microservice architecture and technologies but with significantly less work.\\n\\nNow, that takes the edge off making the right decision on where to compute your data transformations and turns the balancing act into a straightforward process: You can let the DataSQRL optimizer handle it for you, experiment with different allocations, and iterate quickly as your application evolves. DataSQRL makes the implementation of your event-driven microservices and streaming applications faster, easier, and more cost-effective.\\n\\nTake a look at the [DataSQRL tutorial](https://www.datasqrl.com/docs/getting-started/quickstart/) to learn more about DataSQRL and [join us on Discord](https://discord.gg/49AnhVY2w9) if you need any help. If you\'re looking to empower your team to build event-driven microservices like the pros, [we\'re here to help](https://www.datasqrl.com/services/) you hit the ground running."},{"id":"temporal-join","metadata":{"permalink":"/blog/temporal-join","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-07-10-temporal-join.md","source":"@site/blog/2023-07-10-temporal-join.md","title":"Why Temporal Join is Stream Processing\u2019s Superpower","description":"Stream processing technologies like Apache Flink introduce a new type of data transformation that\u2019s very powerful: the temporal join. Temporal joins add context to data streams while being efficient and fast to execute.","date":"2023-07-10T00:00:00.000Z","formattedDate":"July 10, 2023","tags":[{"label":"Join","permalink":"/blog/tags/join"},{"label":"Flink","permalink":"/blog/tags/flink"},{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":8.36,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"temporal-join","title":"Why Temporal Join is Stream Processing\u2019s Superpower","authors":["matthias"],"tags":["Join","Flink","DataSQRL"]},"prevItem":{"title":"To Preprocess or to Query, that\u2019s the Question!","permalink":"/blog/preprocess-or-query"},"nextItem":{"title":"Why Apache Flink and GraphQL Are like Peanut Butter and Jelly","permalink":"/blog/flink-graphql-peanut-butter-jelly"}},"content":"<head>\\n  <meta property=\\"og:image\\" content=\\"/img/dev/temporal_join.png\\" />\\n  <meta name=\\"twitter:image\\" content=\\"/img/dev/temporal_join.png\\" />\\n</head>\\n\\nStream processing technologies like Apache Flink introduce a new type of data transformation that\u2019s very powerful: the temporal join. Temporal joins add context to data streams while being efficient and fast to execute.\\n\\n<img src=\\"/img/dev/temporal_join.svg\\" alt=\\"Temporal Join >\\" width=\\"30%\\"/>\\n\\nThis article introduces the temporal join, compares it to the traditional inner join, explains when to use it, and why it is a secret superpower.\\n\\nTable of Contents:\\n* [The Join: A Quick Review](#review)\\n* [The Temporal Join: Linking Stream and State](#tempjoin)\\n* [Temporal Join vs Inner Join](#tempinner)\\n* [Why Temporal Joins are Fast and Efficient](#efficient)\\n* [Temporal Joins Made Easy to Use](#easy)\\n* [Summary](#summary)\\n\\n\x3c!--truncate--\x3e\\n\\n## The Join: A Quick Review {#review}\\n\\nLet\'s take a quick detour down memory lane and revisit the good ol\' join operation. That trusty sidekick in your SQL utility belt helps you link data from two or more tables based on a related column between them.\\n\\nSuppose we are operating a factory with a number of machines that roast and package coffee. We place sensors on each machine to monitor the temperature and detect overheating.\\n\\nWe keep track of the sensors and machines in two database tables.\\n\\nThe `Sensor` table contains the serial number and machine id that the sensor is placed on.\\n\\n| id | serialNo | machineid |\\n|----|----------|-----------|\\n| 1  | X57-774  | 501       |\\n| 2  | X33-453  | 203       |\\n| 3  | X54-554  | 501       |\\n\\nThe `Machine` table contains the name of each machine.\\n\\n| id    | name           |\\n|-------|----------------|\\n| 203   | Iron Roaster   |\\n| 501   | Gritty Grinder |\\n\\nTo identify all the sensors on the machine \u201cIron Roaster\u201d we use the following SQL query which joins the `Sensor` and `Machine` tables:\\n```sql\\nSELECT s.id, s.serialNo FROM Sensor s \\n    JOIN Machine m ON s.machineid = m.id \\n    WHERE m.name = \u201cIron Roaster\u201d\\n```\\n\\nWhy are joins as important as your morning coffee? Without it, your data tables are like islands, isolated and lonely. Joins bring them together, creating meaningful relationships between data, and enriching data records with context to see the bigger picture.\\n\\nBy default, databases execute joins as **inner** joins which means only matching records are included in the join.\\n\\nSo, now that we\'ve refreshed our memory about the classic join, let\'s dive into the exciting world of temporal joins in stream processing systems like Apache Flink.\\n\\n## The Temporal Join: Linking Stream and State {#tempjoin}\\n\\n<img src=\\"/img/blog/delorean.jpeg\\" alt=\\"Temporal Join DeLorean >\\" width=\\"40%\\"/>\\n\\nPicture this: you\'re a time traveler. You have the power to access any point in time, past or future, at your will. Now, imagine that your data could do the same. Enter the Temporal Join, the DeLorean of data operations, capable of taking your data on a time-traveling adventure.\\n\\nA Temporal Join is like a regular join but with a twist. It allows you to join a stream of data (the time traveler) with a versioned table (the timeline) based on the time attribute of the data stream. This means that for each record in the stream, the join will find the most recent record in the versioned table that is less than or equal to the stream record\'s time.\\n\\nThe versioned table is a normal state table where we keep track of data changes over time. That is, we keep older versions of each record around to allow the stream to match the correct version in time. Like time travel, temporal joins can make your head spin a bit. Let\u2019s look at an example to break it down.\\n\\n## Temporal Join vs Inner Join {#tempinner}\\n\\nBack to our coffee roasting factory, we collect the temperature readings from each sensor in a data stream. \\n\\n| timestamp           | sensorid | temperature |\\n|---------------------|----------|-------------|\\n| 2023-07-10T07:11:08 | 1        | 105.2       |\\n| 2023-07-10T07:11:08 | 2        | 83.1        |\\n| ...                 |          |             |\\n| 2023-07-10T13:25:16 | 1        | 77.8        |\\n| 2023-07-10T13:25:16 | 2        | 83.5        |\\n\\nAnd we want to know the maximum temperature recorded for each machine.\\n\\nEasy enough, let\u2019s join the temperature data stream with the Sensors table and aggregate by machine id:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r INNER JOIN Sensor s \\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nBut here is a problem: What if we moved a sensor from one machine to another during the day? With an inner join, all of the sensor\u2019s readings would be linked to the machine it was last placed on. So, if sensor 1 records a high temperature of 105 degrees in the morning and we move the sensor to the \u201cIron Roaster\u201d machine in the afternoon, then we might see the 105 degrees falsely show up as the maximum temperature for the Iron Roaster. See how time played a trick on our join?\\n\\nAnd this happens whenever we join a data stream with a state table that changes over time, like our sensors that get moved around the factory. What to do? Let\u2019s call the temporal join to our rescue:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading r TEMPORAL JOIN Sensor s \\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nPretty much the same query, just a different join type. Just a heads-up: the syntax for temporal joins in Flink SQL is more complex - we\'ll get to that [later](#easy).\\n\\nAs a temporal join, we are joining each sensor reading with the version of the sensor record at the time of the data stream. In other words, the join not only matches the sensor reading with the sensor record based on the id but also based on the timestamp of the reading to ensure it matches the right version of the sensor record. Pretty neat, right?\\n\\nWhenever you join a data stream with a state that changes over time, you want to use the temporal join to make sure your data is lined up correctly in time. Temporal joins are a powerful feature of stream processing engines that would be difficult to implement in a database.\\n\\n\\n## Why Temporal Joins are Fast and Efficient {#efficient}\\n\\n<img src=\\"/img/external/flink_logo.svg\\" alt=\\"Apache Flink >\\" width=\\"30%\\"/>\\n\\nNot only do temporal joins solve the time-alignment problem when joining data streams with changing state, modern stream processors like Apache Flink are also incredibly efficient at executing temporal joins. A powerful feature with great performance? Sounds too good to be true. Let\u2019s peek behind the stream processing curtain to find out why.\\n\\nIn stream processing, joins are maintained as the underlying data changes over time. That requires the stream engine to hold all the data it needs to update join records when either side of the join changes. This makes inner joins pretty expensive on data streams.\\n\\nConsider our max-temperature query with the inner join: When we join a temperature reading with the corresponding sensor record, and that record changes, the engine has to update the result join record. To do so, it has to store all the sensor readings to determine which join results are affected by a change in a sensor record. This can lead to a lot of updates and hence a lot of downstream computation. It can also cause system failure when there are a lot of temperature readings in our data stream because the stream engine has to store all of them.\\n\\nTemporal joins, on the other hand, can be executed much more efficiently. The stream engine only needs to store the versions of the sensor table that are within the time bounds of the sensor reading data stream. And it only has to briefly store (if at all) the sensor reading records to ensure they are joined with the most up-to-date sensor records. Moreover, temporal joins don\u2019t require sending out a massive amount of updated join records when sensors change placement since the join is fixed in time.\\n\\n## Temporal Joins Made Easy to Use {#easy}\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"30%\\"/>\\n\\nBecause temporal joins are so powerful, we made them easy to use in DataSQRL. DataSQRL is a compiler for Apache Flink that builds integrated microservices for your event-driven or streaming applications. DataSQRL supports the simplified temporal join syntax shown in the queries above. In addition, DataSQRL defaults to a temporal join whenever you join a state and a stream table on the state table\u2019s primary key. In that way, DataSQRL helps you pick the right join for your data and makes it easy for developers new to stream processing.\\n\\n\\nApache Flink supports temporal joins in Flink SQL using the following syntax:\\n\\n```sql\\nSELECT s.machineid, MAX(r.temperature) AS maxTemp \\nFROM SensorReading AS r  JOIN Sensor \\n    FOR SYSTEM_TIME AS OF SensorReading.timestamp AS s \\n    ON r.sensorid = s.id GROUP BY s.machineid\\n```\\n\\nYou need to be careful that the join column for the state table is the primary key of that table and that you set the timestamp for the SensorReading table. DataSQRL does that for you automatically based on watermark.\\n\\n## Time to Wrap Up This Temporal Journey {#summary}\\n\\nWe\'ve reached the end of our time-traveling adventure through the universe of temporal joins. We\'ve seen how they\'re like the DeLorean of data operations, zipping us back and forth through time to make sure our data matches up just right. We\'ve also compared them to the good ol\' inner join.\\n\\nTemporal joins help us avoid the pitfalls of time-alignment problems when joining data streams with changing state. They\'re also super efficient, making them a great choice for high-volume, real-time data processing.\\n\\nAnd that\u2019s why the temporal join is stream processing\'s secret superpower.\\n\\nDataSQRL makes using temporal joins a breeze. With its simplified syntax and smart defaults, it\'s like having a personal tour guide leading you through the sometimes bewildering landscape of stream processing. Take a look at our [IoT tutorial](/docs/getting-started/tutorials/iot/intro/) to see a complete example of temporal joins in action or take a look at our [extended tutorial](/docs/getting-started/intro/overview) for a step-by-step guide to stream processing including temporal joins. And we are [here to help](/community) if you have any questions.\\n\\nHappy data time-traveling, folks!"},{"id":"flink-graphql-peanut-butter-jelly","metadata":{"permalink":"/blog/flink-graphql-peanut-butter-jelly","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-06-27-flink-graphql.md","source":"@site/blog/2023-06-27-flink-graphql.md","title":"Why Apache Flink and GraphQL Are like Peanut Butter and Jelly","description":"In the world of data-driven applications, Apache Flink is a powerful tool that transforms streams of raw data into valuable results. But how do you make these results accessible to users, customers, or consumers of your application? Most often, we found the answer to that question was: GraphQL. GraphQL gives users a flexible way to query for data, makes it easy to ingest events, and supports pushing data updates to the user in real-time.","date":"2023-06-27T00:00:00.000Z","formattedDate":"June 27, 2023","tags":[{"label":"GraphQL","permalink":"/blog/tags/graph-ql"},{"label":"Flink","permalink":"/blog/tags/flink"},{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":8.47,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"flink-graphql-peanut-butter-jelly","title":"Why Apache Flink and GraphQL Are like Peanut Butter and Jelly","authors":["matthias"],"tags":["GraphQL","Flink","DataSQRL"]},"prevItem":{"title":"Why Temporal Join is Stream Processing\u2019s Superpower","permalink":"/blog/temporal-join"},"nextItem":{"title":"Simplifying Apache Flink Application Development with DataSQRL","permalink":"/blog/simplifying-flink-app-development"}},"content":"In the world of data-driven applications, Apache Flink is a powerful tool that transforms streams of raw data into valuable results. But how do you make these results accessible to users, customers, or consumers of your application? Most often, we found the answer to that question was: GraphQL. GraphQL gives users a flexible way to query for data, makes it easy to ingest events, and supports pushing data updates to the user in real-time.\\n\\n<img src=\\"/img/blog/flink_graphql.svg\\" alt=\\"Flink hearts GraphQL >\\" width=\\"40%\\"/>\\n\\nIn this blog post, we\u2019ll discuss what GraphQL is and why it is a good fit for Flink applications. Like peanut butter and jelly, Flink and GraphQL don\u2019t seem related but the combination is surprisingly good.\\n\\nTable of Contents:\\n * [How To Access Flink Results?](#access)\\n * [What is GraphQL?](#graphql)\\n * [Benefit #1: Flexible Access for Data APIs](#benefit1)\\n * [Benefit #2: Realtime Data Updates with GraphQL Subscriptions](#benefit2)\\n * [Benefit #3: Simplify Event Ingestion with GraphQL Mutations](#benefit3)\\n * [Downsides of using GraphQL in Flink Applications](#downsides)\\n * [How to Build GraphQL APIs with Flink](#howto)\\n\\n\\n## How To Access Flink Results? {#access}\\n\\nQuick background before we dive into the details. [Apache Flink](https://flink.apache.org/) is a scalable stream processor that can ingest data from multiple sources, integrate, transform, and analyze the data, and produce results in real time. Apache Flink is the brain of your data processing operations.\\n\\n<img src=\\"/img/external/flink_logo.svg\\" alt=\\"Flink Logo >\\" width=\\"30%\\"/>\\n\\nBut Apache Flink cannot make the processed results accessible to users of your application. Flink has an API, but that API is only for administering and monitoring Flink jobs. It doesn\u2019t give outside users access to the result data. In other words, Flink is a brain without a mouth to communicate results externally.\\n\\nTo make results accessible, you have to write them somewhere and expose them through an interface. But how? We have built a number of Flink applications and in most cases, the answer was: write the results to a database or Kafka and expose them through an API. Over the years, our default choice for the API has become GraphQL. Here\u2019s why.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is GraphQL? {#graphql}\\n\\nFirst, let\'s talk about [GraphQL](https://graphql.org/), the data query and manipulation language that\'s been shaking up the API world.\\n\\n<img src=\\"/img/external/graphql_logo.svg\\" alt=\\"GraphQL Logo >\\" width=\\"30%\\"/>\\n\\nGraphQL is a robust alternative to REST for building APIs. It provides a complete and understandable description of your data in the API and gives clients the power to ask for exactly what they need. This means no more over-fetching of data or making multiple requests, which can be an issue with REST.\\n\\nIn contrast to REST, which uses a separate URL for each resource, GraphQL operates over a single endpoint using HTTP. This simplifies the process of making API calls, as you don\'t have to construct multiple URLs. Furthermore, GraphQL uses a type system to describe data, which can make your API calls more predictable and easier to debug.\\n\\nLike REST, GraphQL is mature and well-established with lots of tooling and client libraries for pretty much all programming languages under the sun.\\n\\nOkay, so GraphQL allows you to build APIs that are easy to use. But what makes it such a good match for Apache Flink applications? GraphQL has three key features that unlock the power of Flink\u2019s data processing: flexible queries, subscriptions, and mutations. Let\u2019s look at those in more detail.\\n\\n## 1. Flexible Access for Data APIs {#benefit1}\\n\\nImagine you\'re at an all-you-can-eat buffet, but instead of food, it\'s data. You can pick and choose exactly what you want and how much of it you need. That\'s what GraphQL does for you when it comes to exposing data APIs. Developers can query the processed streaming data however they need it, without being constrained by predefined endpoints.\\n\\nOther API standards like REST or gRPC are like ordering dishes from a menu. You can only access the endpoints that are defined in the API. That\u2019s fine for many applications, but for data APIs, there are many possible combinations of data records users want to query. So, you either define a lot of endpoints or shift the burden onto the user to stitch their desired result sets together. In the end, it\u2019s more work and a lot of food gets wasted.\\n\\nBut wait, there\'s more! GraphQL not only allows you to select the data you need, but it also simplifies the process of combining data from multiple sources. This means you can easily merge data from various microservices and databases, creating a seamless and unified experience for developers.\\n\\nIn a nutshell, GraphQL empowers developers by providing a flexible, powerful, and efficient way to access the data they need.\\n\\n## 2. Realtime Data Updates with GraphQL Subscriptions {#benefit2}\\n\\nRemember those walkie-talkies you used to play with as a kid? You\'d press the button, and your voice would be magically transmitted to your friend\'s walkie-talkie in real-time. Well, GraphQL subscriptions are kind of like that, but for data.\\n\\nSubscriptions allow Flink developers to push real-time data updates to consumers of the API, ensuring that everyone is always up-to-date with the latest information. It\'s like having a walkie-talkie channel dedicated to data updates, so you never miss a beat.\\n\\nBut that\'s not all! GraphQL subscriptions also enable developers to filter and control the data they receive. This means that each consumer can specify exactly what data they want to be updated on, reducing the amount of unnecessary information being transmitted.\\n\\nIn essence, GraphQL subscriptions provide a powerful and efficient way to keep everyone in the loop with real-time data updates. It\'s like having a personal news ticker, tailored specifically to your needs.\\n\\n## 3. Simplify Event Ingestion with GraphQL Mutations  {#benefit3}\\n\\nMutations make it simple to ingest events into your system, streamlining the process and keeping everything running smoothly. GraphQL mutations support complex event payloads that are defined as input types directly in the API specification, making it easy for users of your API to submit data.\\n\\nGraphQL was developed with mobile applications in mind, where the connection between phone and server can be spotty. GraphQL mutations follow an event-centric model to separate the state of the mobile device from the state on the server and ensure continued operations when the connection between the two gets interrupted. That\u2019s a perfect match for event-driven microservices and streaming applications which follow the same model for decoupling.\\n\\nAnd unlike REST, you don\u2019t have to worry about HTTP methods and state management which add a level of complexity you don\u2019t need for event-driven applications.\\n\\nIn summary, GraphQL mutations structure data ingestion and updates as events - just like Flink.\\n\\n## Downsides of using GraphQL in Flink Applications {#downsides}\\n\\nDespite the many advantages of using GraphQL with Apache Flink, it\'s important to note that there are some potential downsides to this approach. Like any technology, GraphQL is not a silver bullet and there are certain challenges that developers may encounter when implementing it in their Flink applications.\\n\\nFirstly, the very flexibility that makes GraphQL so appealing can also present challenges. The ability to request exactly the data you need means that your backend has to be prepared to handle a potentially wide variety of query structures. This can be difficult to implement, and it requires careful consideration of how your database is structured and indexed. We\u2019ve seen a single missing index bring down an entire application because the database became overloaded. Not a fun day.\\n\\nSecondly, while GraphQL\'s subscription model is a powerful tool for delivering real-time updates, it can also be difficult to implement with low latency. Especially in high-volume data environments like those typically handled by Flink. Ensuring that updates are delivered to clients as quickly as possible, without overwhelming the server or the network, can be a complex task that requires careful planning and optimization.\\n\\nHowever, these challenges are not insurmountable. With the right tooling and a thoughtful approach to implementation, it\'s possible to take full advantage of the power and flexibility that GraphQL offers without optimizing the implementation by hand. DataSQRL is a tool that optimizes your Flink jobs as well as generates the database schema and index structures for your GraphQL API, compiling an entire event-driven microservice and saving you a ton of work. More on DataSQRL below.\\n\\n## How to Build GraphQL APIs with Flink {#howto}\\n\\nTo sum up, the combination of Apache Flink and GraphQL provides a potent solution for data-driven applications. Like peanut butter and jelly, these two distinct technologies complement each other and create a powerful synergy. Flink processes the data, and GraphQL exposes it in a flexible, efficient, and real-time manner. GraphQL\u2019s ability to provide flexible access to data APIs, push real-time data updates through subscriptions, and simplify event ingestion with mutations, makes it an ideal interface for Flink\'s data processing capabilities. Together, they enable developers to build robust, efficient, and user-friendly data applications. So, whether you\'re building a real-time analytics platform, a complex event-driven application, or a data-intensive microservice, consider the Flink-GraphQL combo.\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"30%\\"/>\\n\\nBy now you are probably hungry to learn more. It may just be all the food analogies, but if you want to learn how to build GraphQL APIs with Flink then read on.\\n\\nYou can build Flink+GraphQL microservices by hand, but like many things in software, it gets a lot easier with the appropriate tooling. We recommend using [DataSQRL](/) for building GraphQL APIs on top of Flink. DataSQRL compiles the SQL that defines your data processing in Flink and your GraphQL API into a complete microservice that integrates these components efficiently.\\n\\nOf course, we are biased.  But, who knows, DataSQRL might save you a ton of work, time, and frustration.\\n\\nCheck out the [Quickstart tutorial](/docs/getting-started/quickstart) for a quick overview of how to consume and ingest data, process it with Flink, and expose the results through a GraphQL API.\\nLooking for a step-by-step guide to building your own GraphQL APIs with Flink? Check out the [introductory tutorial](/docs/getting-started/intro/overview) for a detailed explanation of all the steps.\\n\\nAnd if you run into any issues or have questions, don\u2019t hesitate to [reach out](/community).\\n\\nHappy coding, and may the match of GraphQL and Apache Flink turn your application into a delicious experience for your users!"},{"id":"simplifying-flink-app-development","metadata":{"permalink":"/blog/simplifying-flink-app-development","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-06-21-simplifying-flink-app-development.md","source":"@site/blog/2023-06-21-simplifying-flink-app-development.md","title":"Simplifying Apache Flink Application Development with DataSQRL","description":"Apache Flink is an incredibly powerful stream processor. But to build a complete application with Flink you need to integrate multiple complex technologies which requires a significant amount of custom code.","date":"2023-06-21T00:00:00.000Z","formattedDate":"June 21, 2023","tags":[{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"},{"label":"Flink","permalink":"/blog/tags/flink"}],"readingTime":4.445,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"simplifying-flink-app-development","title":"Simplifying Apache Flink Application Development with DataSQRL","authors":["matthias"],"tags":["DataSQRL","Flink"]},"prevItem":{"title":"Why Apache Flink and GraphQL Are like Peanut Butter and Jelly","permalink":"/blog/flink-graphql-peanut-butter-jelly"},"nextItem":{"title":"SQRL: Enhancing SQL to a High-Level Data Language","permalink":"/blog/sqrl-high-level-data-language-sql"}},"content":"Apache Flink is an incredibly powerful stream processor. But to build a complete application with Flink you need to integrate multiple complex technologies which requires a significant amount of custom code.\\nDataSQRL is an open-source tool that simplifies this process by compiling SQL into a microservice that integrates Flink, Kafka, Postgres, and API layer. \\n\\n<div style={{float: \'right\', width: \'40%\'}}>\\n<iframe width=\\"100%\\" height=\\"100%\\" src=\\"https://www.youtube.com/embed/mf5q-IdbVQY\\" title=\\"DataSQRL Introduction\\" frameBorder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\" allowFullScreen></iframe>\\n</div>\\n\\nDataSQRL allows you to focus on your application logic without getting bogged down in the details of how to execute your data transformations efficiently across multiple technologies.\\n\\n## The Challenge of Building Applications with Flink\\n\\nWe have built several applications in Flink: recommendation engines, data mesh endpoints, monitoring dashboards, Customer 360 APIs, smart IoT apps, and more. Across those use cases, Flink proved to be versatile and powerful in its ability to instantly analyze and aggregate data from multiple sources. But we found it quite difficult and time-consuming to build applications with Flink.\\n\\n<img src=\\"/img/reference/compiledMicroservice.svg\\" alt=\\"DataSQRL compiled microservice >\\" width=\\"50%\\"/>\\n\\nTo start, you need to learn Flink: the table and datastream API, watermarking, windowing, and all the other stream processing concepts. Flink alone gets our heads spinning. And Flink is just one component of the application.\\n\\nTo build a complete application or microservice, you need Kafka to hold your streaming data and a database like Postgres to query the processed data. On top, you need an API layer that captures input data and provides access to the processed data. Your team must learn, implement, and integrate multiple complex technologies. It takes a village to build a Flink app.\\n\\n## Introducing DataSQRL: A Solution for Streamlined Flink Development\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"20%\\"/>\\n\\nThat\u2019s why we built [DataSQRL](/). DataSQRL compiles the SQL that defines your data processing into an integrated microservice that orchestrates Flink, Kafka, Postgres, and API - saving us a ton of time and headache in the process. Why not let the computer do all the hard work?\\n\\nLet me show you how DataSQRL works by building an IoT monitoring service.\\n\\n\x3c!--truncate--\x3e\\n\\nFirst, we implement the data processing for our monitoring service in SQL.\\n\\n```sql title=metrics.sqrl\\nIMPORT datasqrl.example.sensors.SensorReading; -- Import sensor data\\nIMPORT time.endOfSecond;  -- Import time function\\n/* Aggregate sensor readings to second */\\nSecReading := SELECT sensorid, endOfSecond(time) as timeSec,\\n                     avg(temperature) as temp FROM SensorReading\\n              GROUP BY sensorid, timeSec;\\n/* Get max temperature in last minute */\\nSensorMaxTemp := SELECT sensorid, max(temp) as maxTemp\\n                 FROM SecReading\\n                 WHERE timeSec >= now() - INTERVAL 1 MINUTE\\n                 GROUP BY sensorid;\\n```\\n\\nThis script imports the metrics stream which are temperature readings collected by sensors. DataSQRL treats external data sources like software dependencies that you import, allowing the compiler to handle configuration management, data mapping, and schema evolution for us. We also import a time function we\u2019ll use for aggregation.\\n\\nThen we define two tables: The first table smoothes out the readings by taking the average temperature each second. The SensorMaxTemp table computes the maximum temperature for each sensor over the last minute.\\n\\nNext, we are going to define the API for our monitoring service.  That\u2019s how users of our service query the data.\\nThe API is defined by a GraphQL schema. It specifies the query endpoints and result types.\\n\\n```graphql title=metricsapi.graphqls\\ntype Query {\\n    SecReading(sensorid: Int!): [SecReading!]\\n    SensorMaxTemp(sensorid: Int): [SensorMaxTemp!]\\n}\\n\\n\\ntype SecReading {\\n    sensorid: Int!\\n    timeSec: String!\\n    temp: Float!\\n}\\n\\n\\ntype SensorMaxTemp {\\n    sensorid: Int!\\n    maxTemp: Float!\\n}\\n```\\n\\nNote, how the tables map to query endpoints and types. That\u2019s how it all fits together.\\n\\nAnd don\u2019t worry, you don\u2019t have to write this schema - DataSQRL can generate it for you from the SQL script.\\n\\nDataSQRL compiles the script and GraphQL schema into an integrated microservice with the following command:\\n\\n```bash\\ndocker run --rm -v $PWD:/build datasqrl/cmd compile metrics.sqrl metricsapi.graphqls\\n```\\n\\nIt also generates a docker-compose template to stand up the entire service.\\n\\n```bash\\n(cd build/deploy; docker compose up)\\n```\\n\\nWe can now interact with the API and try it out by opening [http://localhost:8888/graphiql/](http://localhost:8888/graphiql/?query=query%20MaxTemp%20%7B%0A%20%20SensorMaxTemp%20%7B%0A%20%20%20%20sensorid%0A%20%20%20%20maxTemp%0A%20%20%7D%0A%7D%0A&operationName=MaxTemp).\\n\\n## DataSQRL Does the Work for You\\n\\n\\nPretty simple, right? And the best part is that DataSQRL compiles deployment artifacts for each component that you can inspect and deploy anywhere. There is no magic or black box.\\nFor example, DataSQRL compiles a Flink jar you can execute on an existing Flink cluster or Flink managed service.\\n\\n<img src=\\"/img/generic/undraw_launch.svg\\" alt=\\"DataSQRL Does the Work >\\" width=\\"30%\\"/>\\n\\nMost importantly, consider all the work we didn\u2019t have to do. No data source configuration, watermark setting, Kafka integration, database schema definition, index structure selection, API implementation, and so on. DataSQRL compiles all that for you.\\n\\nDataSQRL also supports inserts to ingest events, subscriptions to push data updates in real-time to the client, and exporting data to Kafka topics or downstream systems. Take a look at the [Quickstart tutorial](/docs/getting-started/quickstart) which shows you how to do that - it only takes a few minutes.\\n\\nDataSQRL is an [open-source project](https://github.com/DataSQRL/sqrl). If you like the idea of DataSQRL, have questions, or need help building your streaming application, [don\u2019t hesitate to reach out](/community).\\n\\nTo sum up, DataSQRL is a tool for simplifying the development of Apache Flink applications by automating the integration of various technologies and allowing developers to focus on their application logic. It makes Flink accessible to lazy developers like us.\\n\\nHave fun building applications with Flink!"},{"id":"sqrl-high-level-data-language-sql","metadata":{"permalink":"/blog/sqrl-high-level-data-language-sql","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-05-22-sqrl-high-level-data-language-sql.md","source":"@site/blog/2023-05-22-sqrl-high-level-data-language-sql.md","title":"SQRL: Enhancing SQL to a High-Level Data Language","description":"When creating data-intensive applications or services, your data logic (i.e. the code that defines how to process the data) gets fragmented across multiple data systems, languages, and mental models. This makes data-driven applications difficult to implement and hard to maintain.","date":"2023-05-22T00:00:00.000Z","formattedDate":"May 22, 2023","tags":[{"label":"SQRL","permalink":"/blog/tags/sqrl"},{"label":"community","permalink":"/blog/tags/community"}],"readingTime":7.505,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"sqrl-high-level-data-language-sql","title":"SQRL: Enhancing SQL to a High-Level Data Language","authors":["matthias"],"tags":["SQRL","community"]},"prevItem":{"title":"Simplifying Apache Flink Application Development with DataSQRL","permalink":"/blog/simplifying-flink-app-development"},"nextItem":{"title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","permalink":"/blog/lets-uplevel-database-datasqrl"}},"content":"When creating data-intensive applications or services, your data logic (i.e. the code that defines how to process the data) gets fragmented across multiple data systems, languages, and mental models. This makes data-driven applications difficult to implement and hard to maintain.\\n\\nSQRL is a high-level data programming language that compiles into executables for all your data systems, so you can implement your data logic in one place. SQRL adds support for data streams and relationships to SQL while maintaining its familiar syntax and semantics.\\n\\n## Why Do We Need SQRL?\\n\\n<img src=\\"/img/reference/reactive_data_layer.svg\\" alt=\\"Data Layer of data-driven application >\\" width=\\"30%\\"/>\\n\\nThe data layer of a data-driven application comprises multiple components: There\u2019s the good ol\u2019 database for data storage and queries, a server for handling incoming data and translating API requests into database queries, a queue/log for asynchronous data processing, and a stream processor for pre-processing and writing new data to the database. Consequently, your data processing code becomes fragmented across various systems, technologies, and languages.\\n\\n\x3c!--truncate--\x3e\\n\\nFor example, consider a project I was once working on. We were building a data service integrating customer data from various silos into a data API for a mobile app. The objective was to provide customers with an integrated view of their service and billing history, support requests, profile information, etc. This is a typical \\"Customer 360\xb0\\" challenge many large organizations face when customer data is spread across numerous operational systems.\\n\\nThe data layer of that service consisted of a bunch of scripts ingesting customer data from CDC (change-data-capture) streams, a SQL database for data persistence, and a Java-based API server.\\n\\nThe data logic of this application was trivial: some translation of billing codes and aggregation of service items, but mostly it was straight-forward mapping of data. Yet, it took the team multiple months to build a prototype because of all the code fragmentation and glue code we had to write to stitch the components together. Integration testing was a big pain. And a simple sprint ticket to add a customer field took a week to implement and test.\\n\\n<img src=\\"/img/blog/tower-of-babel.jpg\\" alt=\\"The Tower of Babel >|\\" width=\\"35%\\"/>\\n\\nThe worst part was all the miscommunication. See, each component of the data layer has a different mental model.\\nFor the data ingestion and pre-processing, the developers thought in terms of events and streams. For the database modeling and querying, the developers thought in terms of rows and tables. And for the API implementation, the developers thought in terms of objects and classes.\\n\\nIt felt like we were building the Tower of Babel. Everybody was speaking a different language and we couldn\u2019t understand each other. But with a twist: We thought we understood each other until it was time to integrate the components and we discovered a mismatch in how we represented the data. That\u2019s a type of punishment not even a jealous God will dish out.\\n\\nTo save ourselves from this tedious work and mental gymnastics, we built SQRL as a high-level data programming language for implementing the data logic of your application in one place.\\n\\n## Introducing SQRL\\n\\nSQRL enhances SQL. If you\u2019ve used SQL before, we hope that you find it easy to pick up SQRL. And if not, there is always ChatGPT \ud83d\ude1c.\\n\\nLet\u2019s take a look at a SQRL script implementing a Customer 360\xb0 API that integrates and aggregates customer and order data:\\n\\n```sql\\nIMPORT datasqrl.seedshop.Orders;\\nIMPORT datasqrl.seedshop.Customers;\\nIMPORT time.*;\\n\\n/* Clean orders data and compute subtotals */\\nOrders.items.discount := discount?0.0;\\nOrders.items.total    := quantity * unit_price - discount;\\nOrders.totals         := SELECT sum(total) as price,\\n                         sum(discount) as saving FROM @.items;\\n/* Deduplicate customer CDC stream */\\nCustomers := DISTINCT Customers ON id ORDER BY timestamp DESC;\\n/* Create relationship between Customers and Orders */\\nCustomers.purchases := JOIN Orders ON Orders.customerid = @.id;\\n/* Aggregate customer spending by state */\\nCustomers.spending := SELECT state, sum(t.price) AS spend,\\n                             sum(t.saving) AS saved\\n                      FROM @.purchases.totals t GROUP BY state;\\n```\\n\\nThis script imports customer data and order streams. It processes data in multiple steps, culminating in an aggregated spending analysis by state.\\n\\nAnd that\u2019s all you have to implement to get a functioning customer 360\xb0 API. DataSQRL compiles this script into executables for all your data systems and handles data mapping and schema synchronization between them.\\n\\n## SQRL Features Overview\\n\\n### Simple Syntax\\n\\nThe first thing you notice is the syntactic sugar that SQRL adds to SQL.\\n\\nIt allows you to define the data sources that you are importing into your script so that a package manager can handle data access configuration and schema management.\\n\\nIt uses the `:=` assignment operator to define new tables and allows incremental column definitions.\\n\\nThe goal is to make SQRL feel a little more like a development language where you build your data logic as a sequence of small, incremental steps instead of writing one massive query.\\n\\n### Nested Data\\n\\nNested data, like JSON documents, is ubiquitous in data-driven applications. It\u2019s how we exchange data. It\u2019s how we expose data in APIs.\\n\\nSQRL provides native support for nested data by representing it as child tables, accessed through the familiar \\".\\" dot notation.\\n\\nIn the example, we sum up the price and saving for all items in an order:\\n```sql\\nOrders.totals := SELECT sum(total) as price, sum(discount) as saving FROM @.items;\\n```\\n\\nThere are a couple of things happening here:\\n\\n- We define a new nested table in `Orders` called `totals` that contains the aggregates\\n- The `FROM` clause `@.items` selects the items from **each** order. The special table handle `@` refers to the parent table in the local context, i.e. `Orders` in this example.\\n\\nBeing able to write queries within a nested context makes it possible to process tree-structured data within SQL.\\n\\nFor example, when we define the `totals` column for each item in an order, we can refer to the other columns of `items` within the local context:\\n\\n```sql\\nOrders.items.total := quantity * unit_price - discount;\\n```\\n\\nNested data support simplifies data consumption from external sources and result data mapping to API calls, eliminating a significant amount of mapping and data transformation code.\\n\\n### Relationships\\n\\nSQRL adds relationships to SQL. You can define relationship columns on tables that relate to rows in other tables using the familiar JOIN syntax.\\n\\n```sql\\nCustomers.purchases := JOIN Orders ON Orders.customerid = @.id;\\n```\\n\\nMaking relationships explicit in SQL simplifies joins and adds structure to the data that is exposed in the API without separate mapping logic.\\n\\nFor example, the `FROM` clause of the spending analysis query uses the relationship expression `@.purchases.totals` to select from the nested `totals` table of the purchase orders for each customer. It eliminates a double-join and makes the query easier to read.\\n\\nSupport for relationships and nested data makes it convenient to handle inter-related data and bridges the gap between the relational data model and the tree or object-relationship structure we use in our APIs and applications.\\n\\n### Stream Processing\\n\\n<img src=\\"/img/blog/data_stream.jpg\\" alt=\\"Matrix Data Stream >|\\" width=\\"40%\\"/>\\n\\nSQRL introduces support for stream tables to ingest external data streams and react to data changes. Data streams are an important part of data-driven applications. It\u2019s how we consume data from other systems or applications and communicate changes in data to subscribers.\\n\\nUnlike normal SQL tables where records can change over time, a stream table has immutable records that are fixed in time. As we saw with the orders stream in our example, SQRL makes it easy to process stream data in steps.\\n\\nSQRL has operators to convert between stream tables and state tables. Our customer 360\xb0 script uses the `DISTINCT` operator to convert a CDC stream into a state table. The `STREAM` operator creates a change stream from a state table, so you can react to changes in state.\\n\\nIn addition, SQRL overloads the `JOIN` operator to support time-consistent joins between state and stream tables. For example, consider the join between the `Customers` and `Orders` tables in the spending analysis query. We want to join the `Orders` stream with the state of the `Customers` table **at the time** of a particular order, so that we aggregate by the state that the customer lived in when the order was placed. If we had used an `INNER JOIN`, the state would update every time the customer moved and the query would aggregate all orders under the state the customer currently lives in.\\n\\nMaking stream tables a first-class citizen in SQL allows us to process stream data, react to changes in data, and bridge the mental model between the set semantics of the relational world and the event orientation of streams.\\n\\nTake a look at the [documentation](/docs/getting-started/intro/sqrl) for a more detailed rundown of all the features SQRL adds to SQL.\\n\\n## Help Us Design SQRL\\n\\nTo take SQRL for a spin and learn how to build data-driven applications, we recommend you start with the [Quickstart tutorial](/docs/getting-started/quickstart). If you have questions, we are happy to answer them on [our Discord](https://discord.gg/49AnhVY2w9).\\n\\nSQRL is still young, and we would love to hear [your feedback](https://discord.gg/49AnhVY2w9) on the language to shape its future."},{"id":"lets-uplevel-database-datasqrl","metadata":{"permalink":"/blog/lets-uplevel-database-datasqrl","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-05-15-lets-uplevel-database-datasqrl.md","source":"@site/blog/2023-05-15-lets-uplevel-database-datasqrl.md","title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","description":"We need to make it easier to build data-driven applications. Databases are great if all your application needs is storing and retrieving data. But if you want to build anything more interesting with data - like serving users recommendations based on the pages they are visiting, detecting fraudulent transactions on your site, or computing real-time features for your machine learning model - you end up building a ton of custom code and infrastructure around the database.","date":"2023-05-15T00:00:00.000Z","formattedDate":"May 15, 2023","tags":[{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"},{"label":"community","permalink":"/blog/tags/community"}],"readingTime":4.79,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"lets-uplevel-database-datasqrl","title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","authors":["matthias"],"tags":["DataSQRL","community"]},"prevItem":{"title":"SQRL: Enhancing SQL to a High-Level Data Language","permalink":"/blog/sqrl-high-level-data-language-sql"},"nextItem":{"title":"DataSQRL 0.1: A SQRL is born","permalink":"/blog/datasqrl-01-release"}},"content":"**We need to make it easier to build data-driven applications.** Databases are great if all your application needs is storing and retrieving data. But if you want to build anything more interesting with data - like serving users recommendations based on the pages they are visiting, detecting fraudulent transactions on your site, or computing real-time features for your machine learning model - you end up building a ton of custom code and infrastructure around the database.\\n\\n<a href=\\"https://www.youtube.com/watch?v=m5uYtBFSmUs&ab_channel=DataSQRL\\" target=\\"_blank\\">\\n<img src=\\"/img/blog/uplevel_play_image.jpg\\" alt=\\"Watch the video version >|\\" width=\\"50%\\"/>\\n</a>\\n\\nYou need a queue like Kafka to hold your events, a stream processor like Flink to process data, a database like Postgres to store and query the result data, and an API layer to tie it all together.\\n\\nAnd that\u2019s just the price of admission. To get a functioning data layer, you need to make sure that all these components talk to each other and that data flows smoothly between them. Schema synchronization, data model tuning, index selection, query batching \u2026 all that fun stuff.\\n\\nThe point is, you need to do a ton of data plumbing if you want to build a data-driven application. All that data plumbing code is time-consuming to develop, hard to maintain, and expensive to operate.\\n\\nWe need to make building with data easier. That\u2019s why we are sending out this call to action to uplevel our database game. **Join us in figuring out how to simplify the data layer.**\\n\\nWe have an idea to get us started: Meet DataSQRL.\\n\\n\x3c!--truncate--\x3e\\n\\n<img src=\\"/img/full_squirrel.svg\\" alt=\\"DataSQRL >\\" width=\\"30%\\"/>\\n\\n\\n## Introducing DataSQRL\\n\\nDataSQRL is a build tool that compiles your application\u2019s data layer from a high-level data development language, dubbed SQRL.\\n\\nOur goal is to create a new abstraction layer above the low-level languages often used in data layers, allowing a compiler to handle the tedious tasks of data plumbing, infrastructure assembly, and configuration management.\\n\\nMuch like how you use high-level languages such as Javascript, Python, or Java instead of Assembly for software development, we believe a similar approach should be used for data. \\n\\nSQRL is designed to be a developer-friendly version of SQL, maintaining familiar syntax while adding features necessary for building data-driven applications, like support for nested data and data streams.\\n\\nCheck out this simple SQRL script to build a recommendation engine from clickstream data.\\n\\n```sql\\nIMPORT datasqrl.example.clickstream.Click;  -- Import data\\n/* Find next page visits within 10 minutes */\\nVisitAfter := SELECT b.url AS beforeURL, a.url AS afterURL,\\n                     a.timestamp AS timestamp\\n        FROM Click b JOIN Click a ON b.userid=a.userid AND\\n                b.timestamp <= a.timestamp AND\\n                b.timestamp >= a.timestamp - INTERVAL 10 MINUTE;\\n/* Recommend pages that are visited shortly after */\\nRecommendation := SELECT beforeURL AS url, afterURL AS rec,\\n                         count(1) AS frequency FROM VisitAfter\\n        GROUP BY url, rec ORDER BY url ASC, frequency DESC;\\n```\\n\\nThis little SQRL script imports clickstream data, identifies pairs of URLs visited within a 10-minute interval, and compiles these pairs into a set of recommendations, ordered by the frequency of co-visits.\\n\\n<img src=\\"/img/reference/reactive_data_layer.svg\\" alt=\\"Reactive Data Layer Compiled by DataSQRL >\\" width=\\"30%\\"/>\\n\\nDataSQRL then takes this script and compiles it into an integrated data layer, complete with all necessary data plumbing pre-installed. It configures access to the clickstream. It generates an executable for the stream processor that ingests, validates, joins, and aggregates the clickstream data. It creates the data model and writes the aggregated data to the database. It synchronizes timestamps and schemas between all the components. And it compiles a server executable that queries the database and exposes the computed recommendations through a GraphQL API.\\n\\n**The bottom line: These 9 lines of SQRL code can replace hundreds of lines of complex data plumbing code and save hours of infrastructure setup.**\\n\\nWe believe that all this low-level data plumbing work should be done by a compiler since it is tedious, time-consuming, and error-prone. Let\u2019s uplevel our data game, so we can focus on **what** we are trying to build with data and less on the **how**.\\n\\n## Join Us on this Journey\\n\\n<img src=\\"/img/blog/undraw_collaboration.svg\\" alt=\\"Join DataSQRL Community >\\" width=\\"50%\\"/>\\n\\n\\nWe have the ambitious goal of designing a higher level of abstraction for data to enable millions of developers to build data-driven applications.\\n\\nWe [just released](https://github.com/DataSQRL/sqrl/releases/tag/v0.1.0) the first version of DataSQRL, and we recognize that we are at the beginning of a long, long road. We need your help. If you are a data nerd, like building with data, or wish it was easier, please [join us on this journey](/community). DataSQRL is an open-source project, and all development activity is transparent.\\n\\nHere are some ideas for how you can contribute:\\n\\n* Share your thoughts: Do you have ideas on how we can improve the SQRL language or the DataSQRL compiler? Jump into [our discord](https://discord.gg/49AnhVY2w9) and let us know!\\n* Test the waters: Do you like playing with new technologies? Try out [DataSQRL](/docs/getting-started/quickstart) and let us know if you find any bugs or missing features.\\n* Spread the word: Think DataSQRL has potential? Share this blog post and [star](https://github.com/DataSQRL/sqrl) DataSQRL on [Github](https://github.com/DataSQRL/sqrl). Your support can help us reach more like-minded individuals.\\n* Code with us: Do you enjoy contributing to open-source projects? Dive into [the code](https://github.com/DataSQRL/sqrl) with us and pick up a [ticket](https://github.com/DataSQRL/sqrl/issues).\\n\\nLet\u2019s uplevel our database game. With your help, we can make building with data fun and productive.\\n\\n## More Information\\n\\nYou probably have a ton of questions now. How do I import my own data? How do I customize the API? How do I deploy SQRL scripts to production? How do I import functions from my favorite programming language?\\n\\nThose are all great questions. Check out [datasqrl.com](/) for answers, [join the community](/community) to ask us, or wait for a future blog post where we dive into all of those topics."},{"id":"datasqrl-01-release","metadata":{"permalink":"/blog/datasqrl-01-release","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-05-11-datasqrl-v0.1.md","source":"@site/blog/2023-05-11-datasqrl-v0.1.md","title":"DataSQRL 0.1: A SQRL is born","description":"After two long years of research, development, and teamwork, we\'re excited to announce DataSQRL 0.1! DataSQRL is a tool for building APIs from your data streams and datasets by defining your use case in an SQL.","date":"2023-05-11T00:00:00.000Z","formattedDate":"May 11, 2023","tags":[{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"},{"label":"release","permalink":"/blog/tags/release"}],"readingTime":1.625,"hasTruncateMarker":true,"authors":[{"name":"Daniel Henneberger","title":"CTO of DataSQRL","url":"https://github.com/henneberger","imageURL":"/img/headshots/daniel1.png","key":"daniel"}],"frontMatter":{"slug":"datasqrl-01-release","title":"DataSQRL 0.1: A SQRL is born","authors":["daniel"],"tags":["DataSQRL","release"]},"prevItem":{"title":"Let\'s Uplevel Our Database Game: Meet DataSQRL","permalink":"/blog/lets-uplevel-database-datasqrl"},"nextItem":{"title":"The Two Core Data Problems for Developers: Transactional & Reactive","permalink":"/blog/types-of-data-problems-transactional-reactive"}},"content":"After two long years of research, development, and teamwork, we\'re excited to announce DataSQRL 0.1! [DataSQRL](/) is a tool for building APIs from your data streams and datasets by defining your use case in an SQL.\\n\\n<img src=\\"/img/blog/datasqrlv0.1.jpeg\\" alt=\\"DataSQRL v0.1 release: A SQRL is Born >\\" width=\\"40%\\"/>\\n\\nThis is our first \u201cofficial\u201d release of DataSQRL after many months of testing and bug-fixing. <br />\\nCheck out the [release notes](https://github.com/DataSQRL/sqrl/releases/tag/v0.1.0) on GitHub for a rundown of all the features.\\n\\n\x3c!--truncate--\x3e\\n\\n## Our Vision\\n\\nEvery time we wanted to build a new use case for our application and expose a data API, we found ourselves getting distracted. Distracted by all the orchestration, the technology choices, all the micro-decisions, and the \'plumbing\' that goes into the modern data layer. So we up-leveled the abstraction and kept it simple. We [designed DataSQRL](/docs/getting-started/concepts/why-datasqrl) to handle those nitty-gritty details, so you could stay focused on what truly mattered - building cool things.\\n\\n## Simplicity Through SQL\\n\\nWe\'ve kept [DataSQRL true to SQL](/docs/getting-started/concepts/datasqrl), so it feels familiar and easy to use. We enhanced and modernized the language while maintaining the simplicity of SQL queries. No more wrestling with subqueries, window functions, or repetitive joins - just straightforward SQL.\\n\\n## Flexible APIs\\n\\nOne size doesn\'t fit all when it comes to APIs. We made DataSQRL non-opinionated, giving you the freedom to use your [preferred GraphQL schema](/docs/reference/api/graphql/design) and customize your query patterns with SQRL scripts.\\n\\n## Our Road Ahead\\n\\nWe\'re seeking [your feedback](/community) to help shape the future of DataSQRL. Our current architecture supports a range of platforms, and we\'re working on making it more extensible and useful. Your input is invaluable as we continue to refine and expand DataSQRL\'s capabilities.\\n\\n## Conclusion\\n\\nThe only danger now is that your boss might think he can start coding again. [Join us](/community) as we explore the story behind DataSQRL, its impact on the world of data processing, and the exciting possibilities it holds for the future."},{"id":"types-of-data-problems-transactional-reactive","metadata":{"permalink":"/blog/types-of-data-problems-transactional-reactive","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-05-01-two-types-of-data-problems-transactional-reactive.md","source":"@site/blog/2023-05-01-two-types-of-data-problems-transactional-reactive.md","title":"The Two Core Data Problems for Developers: Transactional & Reactive","description":"Introduction","date":"2023-05-01T00:00:00.000Z","formattedDate":"May 1, 2023","tags":[{"label":"data","permalink":"/blog/tags/data"}],"readingTime":12.825,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"types-of-data-problems-transactional-reactive","title":"The Two Core Data Problems for Developers: Transactional & Reactive","authors":["matthias"],"tags":["data"]},"prevItem":{"title":"DataSQRL 0.1: A SQRL is born","permalink":"/blog/datasqrl-01-release"},"nextItem":{"title":"Hello, World!","permalink":"/blog/welcome"}},"content":"## Introduction\\nEvery developer, whether you build applications or backend services, encounters two distinct types of data problems: transactional and reactive. The need to store and retrieve application state is a quintessential example of a transactional data problem. Conversely, when you\'re processing events or consuming data from external sources, you\'re confronted with a reactive data problem. \\n\\nKnowing which problem you\'re up against is crucial to selecting the right tools from your developer\'s kit. It\u2019s important to determine what type of data problem you are dealing with to choose the right tools and approaches for implementing a solution. After all, using a hammer for a screw job can leave you with more than a few cracks to mend.\\n\\nIn this post, we\'ll guide you on how to differentiate between transactional and reactive data problems and pick the right tools and strategies to solve each of them.\\n\\n\x3c!--truncate--\x3e\\n\\nTable of Contents:\\n* [The Pitfall of Misinterpreting Reactive Problems as Transactional](#pitfall)\\n* [What are Transactional Data Problems?](#what-tx)\\n* [Solving Transactional Data Problems](#solve-tx)\\n* [What are Reactive Data Problems?](#what-rx)\\n* [Solving Reactive Data Problems](#solve-rx)\\n* [Conclusion](#conclusion)\\n\\n## The Pitfall of Misinterpreting Reactive Problems as Transactional {#pitfall}\\n\\n<img src=\\"/img/blog/arrived_logo.png\\" alt=\\"Arrived Logo >\\" width=\\"30%\\"/>\\n\\nLet\'s kick things off with an anecdote from my career. An episode where I mistakenly treated a reactive data problem as a transactional one, resulting in a full-blown application meltdown. Definitely not a shining moment of my career.\\n\\nFlashback to 2011, I was the backend developer for a sprouting startup named \\"Arrived\\". Our vision was to connect people in the real world by using their phone\u2019s location data. That was the time when smartphones started to support geo-fencing and folks thought Foursquare was going to become the next Facebook. Fun times.\\n\\nWe built an iPhone app that allowed users to establish geo-fences and automatically check-in, alerting their connections upon entry. For a brief overview of the app, check out [this brutally hilarious review](https://techcrunch.com/2011/11/10/i-am-a-passenger-and-i-arrive-and-arrived/) from our TechCrunch Disrupt final presentation. While it was soul-crushing at the time, it\'s quite a fun read in retrospect.\\n\\nI implemented the backend of the application as a Java web service, complete with a REST API for user creation, user connection management, and alert dispatch. The API primarily dealt with storing and retrieving user states, connections, geo-fences, and more. These are typical transactional data problems: how to maintain your application state in a durable, reliable, and consistent way. To tackle this, I used MySQL as the database and an object-relational mapping library to translate my Java objects to database rows.\\n\\nAll was sailing smoothly until we decided to implement a social feature to enhance the onboarding experience and boost the app\'s \\"virality\\". This feature uploaded a user\u2019s phone contacts to check if any of their contacts were already using Arrived, suggesting them as potential connections.\\n\\n<img src=\\"/img/blog/arrived_app.png\\" alt=\\"Arrived Mobile App Screenshot >|\\" width=\\"30%\\"/>\\n\\nThat looked like another transactional data problem to me. Or rather, I was oblivious to the existence of other types of data problems and defaulted to it being transactional.\\n\\nThus, I embarked on a path that would eventually lead to disaster. The \\"upload contacts\\" API call I set up did the following: \\n* stored all contact entries in the database, \\n* ran a for-loop to match any entry hashes already in the database, \\n* added a \\"potential connection\\" record to another table in case of a match.\\n\\nTo my credit, the feature worked as intended. I even had a passing test case. But once we launched the feature in production, our database froze.\\n\\nAs it turned out, some users had an expansive social circle with over a thousand contacts. Running a transaction that writes thousands of records and fires off as many read queries on your primary operational database, which also services all your API requests, is a recipe for disaster. Needless to say, the database was not a fan of this idea and promptly crashed.\\n\\nBut my mistake was not a coding error. The code worked fine. The mistake was failing to realize that the \u201ccontacts matching feature\u201d was a reactive data problem, not a transactional one. We were ingesting data from an external source - a user\u2019s contact list - and reacting to it by comparing matches against our existing user base.\\n\\nIn the upcoming sections, we\'ll delve deeper into the differences between transactional and reactive data problems and how to solve them. We\'ll also revisit my reactive data problem and explore how a more informed approach could have saved me from a full-blown, hair-on-fire database crisis.\\n\\nWe will discuss how I could have solved my reactive data problem better and avoided a hair-on-fire database resurrection after we explore transactional and reactive data problems in more detail and how to distinguish between them.\\n\\nAs for \\"Arrived\\", we learned that our most active users were over-vigilant parents monitoring their children, which was not our target audience. Consequently, we had to close shop in less than two years.\\n\\n## What are Transactional Data Problems? {#what-tx}\\n\\nTransactional data problems arise when you need to store and retrieve data concurrently while maintaining consistency. Here, \\"concurrently\\" refers to the simultaneous reading and writing of data by multiple threads or users. The trick is to ensure that data remains consistent throughout this flurry of updates.\\n\\nThere are two forms of inconsistencies we need to avoid. The first relates to upholding application constraints. For instance, if a username is required to be unique, we cannot allow two user records with identical usernames. This would be inconsistent with our application\'s unique username constraint. We may have several such constraints, like \\"account balances can\'t be negative,\\" or \\"each product id in the orders table must correspond to an existing row in the product table.\\"\\n\\nThe second inconsistency type relates to multiple updates triggered by a single request. We want to dodge situations where only some updates are stored. It\'s an all-or-nothing game - we either want all updates to be stored or none at all. For example, a request to transfer $100 from account A to account B requires updating both account balances. If only account A\'s balance is updated while account B\'s update fails, we\'ve got a magical disappearing act of money.\\n\\nEnsuring data consistency while managing concurrent user updates can be quite a challenge. You might encounter scenarios where two users try to register with the same username simultaneously or two users attempt to withdraw from the same account, causing the balance to plummet below zero. Situations like these are why data storage and retrieval often turn into a \\"problem\\" for developers.\\n\\nTransactional data problems typically surface when storing state for applications that multiple users can access concurrently, or when building CRUD APIs.\\n\\n## Solving Transactional Data Problems {#solve-tx}\\n\\nThe panacea for transactional data problems? Databases. Developers harness the power of databases to efficiently handle the concurrency and consistency issues associated with transactional data problems.\\n\\nHowever, databases aren\'t one-size-fits-all. They differ in the types of consistency and concurrency guarantees they offer. If you\'re using any of the popular relational databases (like Postgres, MySQL, SqlServer, Oracle, and Aurora), breathe easy. They\'re likely equipped with all the support you need. For other databases, it\'s worth checking what exactly they support to avoid surprises down the line.\\n\\nAlongside the choice of database, you\'ll also want to equip yourself with a tool that simplifies interactions with the database from your programming language. Wrestling with databases directly can be cumbersome, requiring the use of drivers, query string writing, and data mapping. If you\'re working in an object-oriented programming language, an object-relational mapping layer (or ORM for short) can be your best friend, translating seamlessly between your application and the database. If not, seek out an SDK or database abstraction layer that\'s compatible with your chosen database.\\n\\n## What are Reactive Data Problems? {#what-rx}\\n\\nYou have a Reactive data problem when your data source is outside your application or service\'s control, and you\'re required to respond to the data quickly. Let\u2019s break this down.\\n\\n### Unconstrained Data\\n\\nWhen the data originates from an external source or isn\'t subject to any application constraints, your application does not control the data source. External data sources could include other systems like logs, message queues, files, external databases, or applications. Here, the data pre-exists independently of your control. For instance, a user\'s contact list is an external data source.\\n\\nMoreover, data could be uncontrolled even within your application, provided it\'s free of any constraints. This includes events that occur organically within your application, such as a user clicking a button or visiting a webpage. These events aren\'t within your direct control - they just happen.\\n\\nThis is a stark contrast to transactional data problems, where the key challenge lies in maintaining data consistency amidst concurrent updates.\\n\\n### The Need for Speed\\n\\nAnother characteristic of reactive data problems is the necessity for swift data processing and result generation. This quick reaction is twofold: it must occur shortly after receiving the data, and it involves computational processing of that data.\\n\\nTake the contacts matching feature in Arrived, for instance. The goal was to encourage users to establish connections during the signup process. Consequently, we had to compute the matches within a few seconds - before the user completed the signup and exited the app.\\n\\nHow swift does this reaction need to be to qualify as \\"quick\\"? It varies according to your use case. Customer-facing use cases typically demand reactions within seconds to minutes, tops. For use cases like fraud detection, system automation, or financial transactions, you may need to respond within milliseconds. If the results can wait for hours or even days, it wouldn\'t qualify as quick.\\n\\nThe \\"reaction\\" element primarily involves generating a response to incoming data, which could either serve back to the user or trigger an action. This could mean processing a user\'s shopping cart to suggest other products they might like, analyzing system metrics to detect potential overload, feeding user activity into a machine learning model for a personalized journey, or evaluating if a transaction request is fraudulent. In each instance, we take a piece of data, evaluate it within the application context, and produce a useful response.\\n\\nIn essence, reactive data problems call for quick, efficient responses to one or multiple data sources. The challenges arise from the need to carry out data-intensive computations rapidly, efficiently, and robustly.\\n\\nReactive data problems commonly crop up in use cases such as:\\n* Personalization or recommendation engines\\n* User experience features\\n* Metrics or time-series analysis\\n* Machine learning features\\n* Fraud detection\\n* Cybersecurity and intrusion detection\\n\\n## How do you solve Reactive Data Problems? {#solve-rx}\\n\\nLet\'s circle back to the reactive data problem of the contacts matching feature. My initial solution involved splitting the transaction into several parts, moving some computation to a background thread, fine-tuning the database schema, and writing a hefty amount of SQL. This strategy was time-consuming, fragile, and a nightmare to maintain. A colleague shrugged it off with \u201cI\u2019ve no idea what\u2019s happening here, but I guess it works\u2026\u201d\\n\\nThe problem? When all you have is a hammer, everything looks like a nail. So, I tried hammering that screw into the wall. As expected, it was neither pretty nor productive.\\n\\n<img src=\\"/img/blog/reactive_data_layer_arrived.svg\\" alt=\\"A Reactive Data Layer Architecture >\\" width=\\"40%\\"/>\\n\\nTo solve reactive data problems more effectively, we need to reimagine our data layer architecture. Let\u2019s give our database some company by introducing additional components that make it easier to process data reactively:\\n* **Queue**: Introducing a persisted message queue (or log) into our architecture can ease asynchronous data processing. This robust, scalable tool allows you to write incoming data to the queue and process it when resources are available, significantly simplifying multi-step data processing.\\n* **Stream Processor**: This is a framework dedicated to managing consecutive data processing steps, from data ingestion (reading data off the queue or from external systems) to data transformation, to writing the results to the database. This framework handles all task scheduling and execution, allowing you to focus on the actual processing logic.\\n* **Server**: This component accepts incoming data, writes it to the queue, and serves the processed data from the database. Acting as the entry and exit point for the reactive data layer, the server brings everything together. You can integrate this functionality into an existing API server implementation or create a standalone server to isolate the reactive data use case into its own backend service.\\n\\nTo improve the contact matching feature, we finally adopted a reactive data architecture as illustrated in the diagram above. Here\'s how the data flowed:\\n\\n1. The server received the submitted contacts and wrote the entire data blob to a persisted messaging queue.\\n2. Three asynchronous tasks ran in the processing framework:\\n    1. *Splitter*: This task read an entire contacts list from the queue, divided it into chunks of a maximum of 50 contacts, and wrote the resulting chunks back to the queue under a different topic.\\n    2. *Storer*: This task read contact chunks from the queue and wrote the contact entries as individual records to the database.\\n    3. *Matcher*: This task read contact chunks, matched the contact entries against the user table, and wrote the found matches to the database.\\n3. The database stored contact entries and contact matches in a separate logical database, isolated from the main operational database serving the CRUD API of the Arrived app.\\n4. The server responded to \\"recommended contacts\\" requests during the signup process by running a query against the database that combined all pre-computed matches for the user with matches from checking the user\'s phone number against previously stored contact entries.\\n\\nThis solution was not only efficient and robust but also easier to maintain. Above all, it allowed us to concentrate on enhancing the feature instead of hacking around the database.\\n\\nSo, here\'s the key takeaway: instead of grappling with reactive data problems using a database alone, build a reactive data layer. It will save you considerable time and frustration.\\n\\nIf building a custom data layer seems intimidating, consider checking out DataSQRL. It\'s a tool that constructs reactive data layers for you. We\'ve been developing DataSQRL to assist developers in resolving reactive data problems quickly and efficiently. We would love to hear your feedback!\\n\\n## Conclusion\\n\\n| Type of Data Problem      | Transactional                 | Reactive                                  |\\n|---------------------------|-------------------------------|-------------------------------------------|\\n| Response time expectation | Milliseconds                  | Milliseconds to minutes                   |\\n| Main challenge            | Consistency under concurrency | Quick Reactions                           |\\n| Source of Data            | Maintained by application     | External or events                        |\\n| Consistency Requirements  | Data constraints & atomicity  | Synchronization in time                   |\\n| **Data Layer Solution**   | **Database + ORM**            | **Queue + Processor + Database + Server** |\\n\\nTransactional data problems arise when your application requires concurrent storage and retrieval of consistent data. On the other hand, reactive data problems occur when your application needs to quickly respond to data from external sources or events. Recognizing the distinction between these two types of data issues is crucial to implementing the most effective solution.\\n\\nFor transactional data problems, a data layer comprising a database and an Object-Relational Mapping (ORM) tool is often the best solution. On the contrary, reactive data problems are more efficiently addressed with a data layer that includes a queue, stream processor, database, and server. Understanding these distinctions and applying the appropriate solutions can significantly improve the efficiency and robustness of your data layer."},{"id":"welcome","metadata":{"permalink":"/blog/welcome","editUrl":"https://github.com/DataSQRL/datasqrl.github.io/edit/main/blog/2023-01-26-welcome.md","source":"@site/blog/2023-01-26-welcome.md","title":"Hello, World!","description":"\\" width=\\"40%\\"/>","date":"2023-01-26T00:00:00.000Z","formattedDate":"January 26, 2023","tags":[{"label":"DataSQRL","permalink":"/blog/tags/data-sqrl"}],"readingTime":1.76,"hasTruncateMarker":true,"authors":[{"name":"Matthias Broecheler","title":"CEO of DataSQRL","url":"https://github.com/mbroecheler","imageURL":"/img/headshots/matthias1.png","key":"matthias"}],"frontMatter":{"slug":"welcome","title":"Hello, World!","authors":["matthias"],"tags":["DataSQRL"]},"prevItem":{"title":"The Two Core Data Problems for Developers: Transactional & Reactive","permalink":"/blog/types-of-data-problems-transactional-reactive"}},"content":"<img src=\\"/img/generic/undraw_launch.svg\\" alt=\\"Launching DataSQRL >\\" width=\\"40%\\"/>\\n\\nWe are excited to launch [DataSQRL](/about) with the mission to help developers and organizations build with data.\\n\\nCollectively, we have spent decades building or helping others build data services. We have seen many struggles, failures, and piles of money being thrown out the window and figured that there must be a better way. We started DataSQRL to find it.\\n\\nWe believe that the technologies used to build data services are too complex and that the engineering processes used to build them are broken. Here is how we plan to fix these issues.\\n\\n\x3c!--truncate--\x3e\\n\\nWe developed [DataSQRL](/) which  compiles a developer-friendly version of SQL into a fully integrated and optimized data pipeline and API server. It takes care of all the laborious plumbing, data massaging, and stitching together of technologies that makes building data services so harrowing. Check out [this short tutorial](/docs/getting-started/quickstart) to see how it works - it only takes a few minutes to build an end-to-end data service.\\n\\nIn addition, we are refining a value-focused process for implementing data services that we have developed over the years while working with development teams and organizations. The basic idea is to apply the same software engineering principles that have proven to be successful to implementing data services. That means you don\'t need a dedicated team of specialists to implement data services and can keep your customers and stakeholders in the feedback loop. [Click here](/docs/process/intro) to learn more about our process.\\n\\nThat\'s our starting point for enabling developers to build successful data services quickly and efficiently. We think we got some good ideas, but have been building data technologies long enough to realize that there is a fine line between innovation and wishful thinking.<br />\\nWe hope that you will join the [DataSQRL community](/community) to share your experience, insights, and opinions to help set us straight.\\n\\nIf you are trying to enable your organization to turn data into valuable data services, consider [working with us](/services) and [get in touch](/contact).\\n\\nWe are excited to be on this journey and hope you will join us. Let\'s build with data together."}]}')}}]);